---
layout: post
category: cstheoryrss
title: "arXiv: Computational Complexity: The Serial Scaling Hypothesis"
date: 2025-07-18T00:00:00
---

**Authors:** [Yuxi Liu](https://dblp.uni-trier.de/search?q=Yuxi+Liu), [Konpat Preechakul](https://dblp.uni-trier.de/search?q=Konpat+Preechakul), [Kananart Kuwaranancharoen](https://dblp.uni-trier.de/search?q=Kananart+Kuwaranancharoen), [Yutong Bai](https://dblp.uni-trier.de/search?q=Yutong+Bai)

While machine learning has advanced through massive parallelization, we
identify a critical blind spot: some problems are fundamentally sequential.
These "inherently serial" problems-from mathematical reasoning to physical
simulations to sequential decision-making-require dependent computational steps
that cannot be parallelized. Drawing from complexity theory, we formalize this
distinction and demonstrate that current parallel-centric architectures face
fundamental limitations on such tasks. We argue that recognizing the serial
nature of computation holds profound implications on machine learning, model
design, hardware development. As AI tackles increasingly complex reasoning,
deliberately scaling serial computation-not just parallel computation-is
essential for continued progress.

[Read original post](http://arxiv.org/abs/2507.12549v1)
