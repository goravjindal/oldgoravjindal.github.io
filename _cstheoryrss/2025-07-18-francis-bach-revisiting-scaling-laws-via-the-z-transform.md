---
layout: post
category: cstheoryrss
title: "Francis Bach: Revisiting scaling laws via the z-transform"
date: 2025-07-18T09:47:44
---

In the last few years, we have seen a surge of empirical and theoretical works about “scaling laws”, whose goals are to characterize the performance of learning methods based on various problem parameters (e.g., number of observations and parameters, or amount of compute). From a theoretical point of view, this marks a renewed interest in asymptotic equivalents—something the machine learning community had mostly moved away from (and, let’s be honest, kind of looked down on) in favor of non-asymptotic bounds. The two types of bounds have their own pros and cons, and I value both of them, but on a personal note, after having spent close to twenty years in proving (sometimes tedious) non-asymptotic results, proving asymptotic results is particularly rewarding, especially when you experience that “aha!” moment where the empirical behavior aligns perfectly with the derived expression (see some examples below).

Scaling laws for quadratic optimization require aggregating the effects of all eigenvalues of the Hessian matrix and not considering only the worst one. This was done with Laplace’s method in an [earlier post](https://francisbach.com/scaling-laws-of-optimization/) for gradient descent and partially for Nesterov acceleration. This blog post explores similar asymptotic scaling laws and will consider stochastic algorithms, but there is more to it.

I will use a classical tool from applied mathematics, electrical engineering, and computer science: the *[z-transform](https://en.wikipedia.org/wiki/Z-transform)* (a.k.a. [generating function](https://en.wikipedia.org/wiki/Generating_function)) of a discrete sequence. My goal in this blog post is to present the essentials of the method. For more details, see my recent preprint [[1](https://arxiv.org/pdf/2507.03404)], with lots of potential follow-ups. Note that the z-transform has already been used in the context of stochastic gradient descent for stability analysis [[15](https://ieeexplore.ieee.org/abstract/document/1164493), [20](https://ieeexplore.ieee.org/abstract/document/236504)] and for deriving convergence rates with constant momentum [[22](https://openreview.net/pdf?id=bzaPGEllsjE)].

## z-transform method and final value theorem

**From Abel’s theorem to the finite value theorem.** Given a sequence \((b\_k)\_{k \geqslant 0}\), its [z-transform](https://en.wikipedia.org/wiki/Z-transform) is the function \(B\) defined for a complex argument \(z \in \mathbb{C}\), as \(B(z) = \sum\_{k=0}^\infty b\_k z^k\) (note the change of convention where \(z^{-1}\) is replaced by \(z\)). The behavior of the series defined by \((b\_k)\_{k \geqslant 0}\) can be characterized by the behavior of \(B(z)\) for \(z\) tending to \(1\) while remaining in \([0,1)\). This is the classical [Abel’s theorem](https://en.wikipedia.org/wiki/Abel%27s_theorem): when the series defined by \((b\_k)\_{k \geqslant 0}\) is convergent, then $$\sum\_{k=0}^{+\infty} b\_k = \lim\_{z \to 1^-} B(z).$$

When applied to \(b\_k = a\_k\, – a\_{k-1}\) for \(k \geqslant 1\), with \(b\_0 = a\_0\), then we have \(\sum\_{\ell=1}^k b\_\ell = a\_k\), and \(B(z) = (1\, – z) A(z)\) where \(A\) is the z-transfom of \((a\_k)\_{k \geqslant 0}\). We then obtain the [final value theorem](https://en.wikipedia.org/wiki/Final_value_theorem), which states $$ \lim\_{z \to 1^-} \ ( 1 \, – z) A(z) = \lim\_{k \to +\infty} a\_k.$$

**From limits to asymptotic equivalents.** In order to obtain equivalents of the sequence \((a\_k)\_{k \geqslant 0}\) (and not simply its limit), we can use two additional properties:

* the z-transform of \(((k+1) a\_{k+1})\_{k \geqslant 0}\) is \(A'(z)\), leading to $$ \lim\_{z \to 1^-} \ ( 1 \, – z) A'(z)= \lim\_{k \to +\infty} k a\_k.$$
* To treat non-integer powers, we have the following identity, valid for any real \(\nu > 0\), $$ \lim\_{z \to 1^-} \ ( 1 \,- z)^{\nu} A(z) = \Gamma(\nu) \cdot \lim\_{k \to +\infty} k^{1-\nu} a\_k,$$ where \(\Gamma\) is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function). A classical example is \(a\_k = \frac{1}{k!} \nu(\nu+1) \cdots (\nu+k-1) = \frac{ \Gamma(k+\nu)}{\Gamma(\nu)\Gamma(k+1)} \sim \frac{k^{\nu-1}}{\Gamma(\nu)}\), for which \(A(z) = (1-z)^{-\nu}\).

Combining the last two properties (with an extension to higher-order derivatives), when the two limits exist, we have: $$ \lim\_{z \to 1^-} \ ( 1 \, – z)^{\nu} A^{(\mu)}(z) = \Gamma(\nu)\cdot \lim\_{k \to +\infty} k^{1+\mu-\nu} a\_k.$$ Therefore, the z-transform method consists in deriving asymptotics of the z-transform or its derivatives around \(z = 1^-\). Before showing why this is a simplification, some words on sufficient conditions for the equivalence to hold, through a sequence of so-called [“Tauberian” theorems](https://en.wikipedia.org/wiki/Abelian_and_Tauberian_theorems).

**Tauberian theory.** In order to show that limits and asymptotic equivalents exist, several options are available. When only considering real-valued arguments for the z-transforms, various sufficient conditions have been derived from the work of Hardy and Littlewood [[2](https://scholar.archive.org/work/6div4q4wvrfc5pheqboxzppbju/access/ia_file/crossref-pre-1923-scholarly-works/10.1112%252Fplms%252Fs2-10.1.116.zip/10.1112%252Fplms%252Fs2-18.1.205.pdf)] (the book from Jacob Korevaar [3] is a great source of detailed results). In a nutshell, oscillating sequences such as \(a\_k = (-1)^k\) are the ones where the limits are not equal (or may not exist), and a classical sufficient condition is that there exists a constant \(c\) such that for all \(k \geqslant 1\), \(a\_k \, – a\_{k-1} \leqslant c / k^{2+\mu – \nu}\) (in particular, with \(c = 0\), being decreasing is a sufficient condition). Alternative frameworks can also be considered using complex analysis, which only depends on property of the z-transform \(A(z)\) for complex \(z\) [[4](https://algo.inria.fr/flajolet/Publications/FlOd90b.pdf), [5](https://epubs.siam.org/doi/pdf/10.1137/1016082)].

**Properties of the z-transform**. What makes the z-transform so versatile is a suite of properties that link modifications of the sequence \((a\_k)\_{k \geqslant 0}\) to its z-transform \(A(z)\) (see more details in [[1](https://arxiv.org/pdf/2507.03404), 6]). The most important ones are:

* **Shift:** If \(A(z)\) is the z-transform of the sequence \((a\_k)\_{k \geqslant 0}\), then \(z A(z)\) is the z-transform of the sequence \((0,a\_0,a\_1,\dots)\), that is, shifted by 1. This implies that sequences that satisfy a [linear difference equation](https://en.wikipedia.org/wiki/Linear_recurrence_with_constant_coefficients) lead to *rational* z-transforms.
* **Multiplication by a polynomial:** If \(A\) is the z-transform of \((a\_k)\_{k \geqslant 0}\), then \(z \mapsto z A'(z)\) is the \(z\)-transform of \((k a\_k)\_{k \geqslant 0}\). Thus, when we have a difference equation with rational coefficients (in \(k\)), this leads to an ordinary differential equation for the z-transform.

**Classical use of the z-transform method to derive asymptotic equivalents.** The z-transform method is a classical tool, with already many [applications](https://en.wikipedia.org/wiki/Generating_function#Applications), e.g., in combinatorics, analysis of recurrence relations, queuing theory, signal processing, and control (see [6, [7](https://www2.math.upenn.edu/~wilf/gfology2.pdf)]). In this blog post, I will focus on optimization of quadratic functions.

## Gradient descent and spectral dimension

Like in an [earlier post](https://francisbach.com/scaling-laws-of-optimization/), we consider an idealized quadratic optimization problem in infinite dimension, which can be rewritten as the minimization of $$F(\eta) = \frac{1}{2} \langle \eta-\eta\_\ast, H ( \eta-\eta\_\ast)\rangle, $$ where \(\eta\_\ast\) is a global optimum of \(F\) and \(H\) the positive semi-definite Hessian operator. Gradient descent leads to the recursion $$ \eta\_k = \eta\_{k-1} \, – \gamma F'(\eta\_{k-1}) = \eta\_{k-1} \, – \gamma H ( \eta\_{k-1} \, – \eta\_\ast), $$ where \(\gamma > 0\) is a step-size which we choose so that the operator norm of \(\gamma H\) is less than one (e.g., \(\gamma \leqslant 1/L\), where \(L\) is the largest eigenvalue of the operator \(H\)). With \(\theta\_k = \eta\_k \, – \eta\_\ast\), this leads to the following *linear* iteration $$ \theta\_k = (I \, – \gamma H) \theta\_{k-1} = ( I \, – \gamma H)^k \delta, $$ where \(\delta = \theta\_0 = \eta\_0\, – \eta\_\ast\).

Thus, given a spectral decomposition \(\gamma H = \sum\_{i=1}^{+\infty} \lambda\_i u\_i \otimes u\_i\) with eigenvalues \((\lambda\_i)\_{i \geqslant 1}\) in \([0,1]\) and eigenvectors \((u\_i)\_{i \geqslant 1}\), we get the following performance criterion $$ a\_k = \frac{1}{2\gamma} \sum\_{i=1}^{+\infty} \lambda\_i (1 \, – \lambda\_i)^{2k} \langle u\_i,\delta \rangle^2 = \int\_{0}^1 (1 \, – \lambda)^{2k} d\sigma(\lambda), $$ where $$ d\sigma(\lambda) = \frac{1}{2\gamma} \sum\_{i=1}^{+\infty} \lambda\_i \langle u\_i,\delta \rangle^2 {\rm Dirac}(\lambda|\lambda\_i) $$  
is a weighted spectral measure, as defined in [[8](https://arxiv.org/pdf/1805.08531)] in the gossip context (see this [earlier post](https://francisbach.com/jacobi-polynomials/)), with a z-transform equal to $$ A(z) = \sum\_{k=0}^{+\infty} z^k \int\_0^{1 } (1-\lambda)^{2k} d\sigma(\lambda)  
= \int\_0^{1 } \frac{1}{1 \, – z (1-\lambda)^{2}} d\sigma(\lambda), $$ which is a rational function in \(z\) and \(\lambda\). It can thus be re-written as, using a partial function decomposition in the variable \(\lambda\), as $$A(z) = \frac{1}{2 \sqrt{z} } \int\_0^{1 } \frac{d\sigma(\lambda) }{ \lambda + z^{-1/2} \, – 1 } – \frac{1}{2\sqrt{z}} \int\_0^{1 } \frac{d\sigma(\lambda) }{ \lambda\, – z^{-1/2} – 1 }.$$ We thus get $$A(z) = \frac{1}{2 \sqrt{z} } S( z^{-1/2}-1 ) – \frac{1}{2 \sqrt{z}}S ( – z^{-1/2}-1) ,$$  
where \(\displaystyle S(u) = \int\_0^1 \! \frac{d\sigma(\lambda)}{\lambda + u}\) is the [Stieltjes transform](https://en.wikipedia.org/wiki/Stieltjes_transformation) of the measure \(\sigma\) (see Chapter 8 of [10]), a classical tool in [random matrix theory](https://en.wikipedia.org/wiki/Random_matrix). We see above that what matters in the equivalent of \(S(u)\) around \(u=0\) (since the value at \(-2\) is bounded).

The *spectral dimension* defined by [[8](https://arxiv.org/pdf/1805.08531)] corresponds to a measure whose behavior around zero has density \(c \lambda^{\omega \, – 1}\), or in almost equivalent terms so that \(S(u) \sim c \Gamma(\omega) \Gamma(1-\omega) u^{\omega\, -1}\) around \(u=0\), with a similar expansion for derivatives. This leads directly to a an explicit scaling law in \({1}/{k^\omega}\) which exactly recovers an [earlier post](https://francisbach.com/scaling-laws-of-optimization/) (which was using Laplace method) with a different proof.

The spectral dimension depends explicitly on problem parameters:

* For least-squares regression, as described in an [earlier post](https://francisbach.com/scaling-laws-of-optimization/) and in many papers, this corresponds to \(\lambda\_i \sim L / i^\alpha\) (“capacity” conditions) and \(\langle \delta, u\_i \rangle \sim \Delta / i^{\beta/2}\) (“source” conditions), leading to \(\omega = \frac{\beta-1}{\alpha} + 1 \) assumed to be positive, and \(c = \frac{L \Delta^2 }{2 \alpha (\gamma L)^{\frac{\beta-1}{\alpha}+1}}\). The equivalent is then $$ a\_ k \sim c \frac{\Gamma(\omega)}{(2k)^{\omega}},$$ with nice [connections](https://francisbach.com/scaling-laws-of-optimization/) with non-asymptotic bounds for the same problem.
* For gossip, as shown in [8], we get that \(\omega\) is the underlying dimension of the set on which local messages are sent.

**Summary.** Linear iterations lead to rational z-transforms along all eigenvectors, and then asymptotics of the [Stieltjes transform](https://en.wikipedia.org/wiki/Stieltjes_transformation) (based on the spectral dimension) can be used after partial function decomposition. For gradient descent (a simple linear iteration with constant coefficients), this can be easily achieved in other ways. However, for time-dependent coefficients (Nesterov acceleration) and for stochastic iterations, it will provide a new direct way that avoids lengthy computations.

## Nesterov acceleration vs. heavy-ball

A classical way to accelerate the convergence of the gradient iteration is to use an [extrapolation step](https://en.wikipedia.org/wiki/Gradient_descent#Fast_gradient_methods) due to Nesterov [[11](https://www.mathnet.ru/links/682668bd73f8d8866766f9356da2378e/dan46009.pdf)]. This can be formulated with two sequences (and a first-order recursion) as:  
$$ \begin{array}{rcl} \eta\_{k+1} & = & \zeta\_{k} \, – \gamma F'(\zeta\_{k}) \\ \zeta\_{k+1} & = & \eta\_{k+1} + ( 1 \, – \tau\_{k+1}) ( \eta\_{k+1} \, – \eta\_k), \end{array} $$ and with a single sequence (and a second-order recursion) as $$ \eta\_{k+1} = \eta\_{k} + ( 1 \, – \tau\_{k}) ( \eta\_{k}\, – \eta\_{k-1})\, – \gamma F’ \big[ \eta\_{k} + ( 1 \,- \tau\_{k}) ( \eta\_{k} \, – \eta\_{k-1})\big]. $$

In the original formulation with convex functions, \(\tau\_k\) is chosen asymptotically proportional to \(2\rho/(k+1)\) with \(\rho = 3/2\). For quadratic functions, [[9](https://proceedings.mlr.press/v40/Flammarion15.pdf)] argues that the choice \(\rho = 1\) is more natural as it leads to a simple rescaled reformulation, and [[12](https://link.springer.com/content/pdf/10.1007/s10957-015-0746-4.pdf), [13](https://arxiv.org/pdf/2407.17063)] consider a general \(\rho\). We now show that the z-transform allows us to analyze all integers \(\rho\), with a conjecture that is empirically valid for all \(\rho >0\).

With \(\theta\_k = \eta\_k\, – \eta\_\ast\), we obtain the following recursion for quadratic functions $$ \theta\_{k+1} = ( I- \gamma H) \bigg[ \theta\_{k} + \Big( 1\, – \frac{ 2\rho }{k+2\rho-1} \Big) ( \theta\_{k} \, – \theta\_{k-1}) \bigg]. $$ I show in [[1](https://arxiv.org/pdf/2507.03404)] how the performance measure leads to an explicit rational function in \((z,\lambda)\) of order \(2 \rho\), obtained through differentiations of the z-transform (to take into account the rational depence of \(\tau\_k\)). For \(\rho=1\), we can easily get by hand a partial function decomposition, while for larger integer \(\rho\), this can be readily done by symbolic computation tools such as Mathematica. Overall, the final equivalents for \(a\_k = \frac{1}{2} \langle \eta\_k \, – \eta\_\ast, H ( \eta\_k \, – \eta\_\ast) \rangle\) are essentially (up to logarithmic terms) proportional to \({1}/{k^{\min\{2\omega, \omega + \rho\}}}\). More precisely we get totally explicit equivalents:

* \(\displaystyle a\_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot \frac{\Gamma(\rho-\omega) \Gamma(\omega)}{\Gamma(4\rho-1-2\omega)} \frac{\Gamma(2\rho-1/2-\omega)}{\Gamma(\rho+1/2-\omega)} \frac{2^{2\rho-1}}{4^\omega } \cdot \frac{1}{k^{ 2\omega}}\) for \(\omega \in (0,\rho)\).
* \(\displaystyle a\_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot\frac{1}{2^{2\rho-1}} \cdot \frac{ \log k}{k^{2\rho}}\) for \(\omega = \rho\).
* \(\displaystyle a\_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot \frac{1}{2^{2\rho-1}} \Gamma(\omega-\rho)\cdot \frac{1}{k^{\omega+\rho}}\) for \(\omega > \rho\).

And amazingly, they do match in our experiments even beyond integer \(\rho\), as shown below.

![](https://francisbach.com/wp-content/uploads/2025/07/video_nesterov_blog.gif)

Nesterov acceleration for \(\omega = 1\) and several \(\rho\)’s. Note the classical vanishing oscillations for small \(\rho\). Moreover, the best performance is achieved around \(\rho \approx \omega\).

**What about heavy-ball?** The heavy-ball method is very similar to Nesterov acceleration, with the following iteration $$ \eta\_{k+1} = \eta\_{k} \, – \gamma F'(\eta\_k) + (1-\tau\_{k+1}) ( \eta\_k \,- \eta\_{k-1}),$$ leading to, with the same notations, to $$\theta\_{k+1} = \Big( I- \frac{k}{k+2\rho-1} \gamma H\Big) \theta\_{k} + \Big( 1 \, – \frac{ 2\rho }{k+2\rho-1} \Big) ( \theta\_{k} \, – \theta\_{k-1}).$$ The associated z-transform is also the integral with respect to the spectral measure of a rational function, leading to an asymptotic equivalent around \(z=1^-\). Then, one can plot the actual performance and compare to the equivalent, as shown below.

![](https://francisbach.com/wp-content/uploads/2025/07/heavyball_blog-1024x829.png)

Heavy-ball method with \(\rho=1\) for two values of the spectral dimension \(\omega\). Top: performance \(a\_k\) vs. equivalent \(\bar{a}\_k\), bottom: ratio.

*What’s happening?* The two do not match for \(\omega\) large enough (right plot above). It turns out that oscillations do not vanish, and the sufficient Tauberian conditions are not satisfied. What can then be shown in terms of asymptotic equivalents remains open.

## Stochastic gradient descent (a.k.a. least-mean-squares algorithm)

We now consider minimizing the expexted risk for a linear predictor with square loss: $$ F(\eta) = \frac{1}{2}\mathbb{E} \big[ ( y \, – \langle x, \eta \rangle )^2 \big]$$ (keeping in mind that we can use a feature vector). Single-pass [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) leads to the iteration $$\eta\_k = \eta\_{k-1}\, – \gamma ( \langle x\_k,\eta\_{k-1} \rangle \,- y\_k) x\_k, $$ with \((x\_k,y\_k)\) an independent sample from the distribution defining \(F\). We can write it using a minimizer \(\eta\_\ast\) of \(F\) as $$\eta\_k -\eta\_\ast =( I \, – \gamma x\_k \otimes x\_k) (\eta\_{k-1} \, – \eta\_\ast) + \gamma \varepsilon\_k x\_k, $$ where \(\varepsilon\_k = y\_k \, – \langle x\_k, \eta\_\ast \rangle\) is the optimal residual. We denote \(\theta\_k = \eta\_k \, – \eta\_\ast\). In this context, SGD is referred to as the [least-mean-squares](https://en.wikipedia.org/wiki/Least_mean_squares_filter) (LMS) algorithm [[14](https://ieeexplore.ieee.org/abstract/document/1164914), [15](https://ieeexplore.ieee.org/abstract/document/1164493), [16](https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf), [17](https://proceedings.mlr.press/v38/defossez15.pdf)].

The simplest but incomplete analysis uses that \(\mathbb{E} [ \varepsilon\_k x\_k ] =0\) (which is the optimality condition characterizing \(\varepsilon\_k\)), to get: $$ \mathbb{E}[\theta\_k] = ( I \, – \gamma H) \mathbb{E}[\theta\_{k-1}],$$ leading to the classical linear iteration of gradient descent. However, it does not lead to any result on the performance measure \(\frac{1}{2} \mathbb{E} [ \langle \theta\_k, H \theta\_k \rangle] = \mathbb{E} [ F(\eta\_k)\, – F(\eta\_\ast) ]\).

Traditionally, there are two terms in the analysis: “bias” and “variance” (see a thorough discussion in [[27](https://jmlr.org/papers/volume18/16-335/16-335.pdf)]). For simplicity, let’s assume that \(\varepsilon\_k\) is independent of \(x\_k\), with \(\mathbb{E} [ \varepsilon\_k]=0\) and \(\mathbb{E} [ \varepsilon\_k^2]=\sigma^2\) (not to be confused with the notation for the spectral measure defined above). We then consider the operator $$\Theta\_k = (\eta\_k \, – \eta\_\ast)\otimes (\eta\_k \,- \eta\_\ast) = \theta\_k \otimes \theta\_k, $$ for which we have $$F(\eta\_k) \,- F(\eta\_\ast) = \frac{1}{2} \langle \Theta\_k,H \rangle = \frac{1}{2}{\rm tr}( \Theta\_k H), $$ and $$ \mathbb{E} [ \Theta\_k | \Theta\_{k-1}] = \mathbb{E} \big[ ( I \, – \gamma x\_k \otimes x\_k) \otimes ( I \, – \gamma x\_k \otimes x\_k) \big] \Theta\_{k-1} + \gamma^2 \sigma^2 H,$$ leading to: $$ \mathbb{E} [ \Theta\_k ] = \big( I\, – \gamma H \otimes I \, – \gamma I \otimes H\, + \gamma^2 \mathbb{E} [ x \otimes x \otimes x \otimes x] \big) \mathbb{E} [ \Theta\_{k-1} ] \, + \gamma^2 \sigma^2 H.$$

We consider the operator (defined on self-adjoint operators) $$ T = H \otimes I \, + I \otimes H \, -\, \gamma \mathbb{E} [ x \otimes x \otimes x \otimes x],$$ so that $$\tag{1} \mathbb{E} [ \Theta\_k ] = \big( I\, – \gamma T \big) \mathbb{E} [ \Theta\_{k-1} ] \, + \gamma^2 \sigma^2 H, $$ leading to, after summation of the geometric series (see an [earlier post](https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/)): $$\mathbb{E} [ \Theta\_k ] = \big( I\, – \gamma T \big)^k \Theta\_{0} \, + \gamma \sigma^2 \big[ I- \big( I- \gamma T \big)^k \big] T^{-1} H .$$

A sufficient condition for convergence is that the operator \(I\, – \gamma T\) (as an operator on symmetric operators) has eigenvalues in \([-1,1]\). In [[17](https://proceedings.mlr.press/v38/defossez15.pdf)], we define and characterize the largest step-size with few assumptions. We consider instead specific models for asymptotic computations.

**Simplest model.** In order to obtain simpler formulas, several models have been proposed that are compatible with the Hessian operator \(H\) having eigenvalues \((\lambda\_i)\_{i \geqslant 1}\) and eigenvectors \((u\_i)\_{i \geqslant 1}\). The simplest model [[18](https://ieeexplore.ieee.org/document/236504)] is \(x = r u\_j\) with \(r \in \mathbb{R}\) and \(i \in \mathbb{N}^\ast\) random and independent, with \(\mathbb{E}[r^2] = \sum\_{i \geqslant 1} h\_i = {\rm tr}(H)\), and \(\mathbb{P}(j = i) = h\_i / {\rm tr}(H)\). What makes this model special is that, like in the deterministic case, there are no interactions between eigensubspaces. While this includes one-hot encoded discrete data as recently used in [[19](https://arxiv.org/pdf/2505.19227)], this remains limited compared to the model below (which includes Gaussian data and is often used in random matrix theory).

**Simplified high-dimensional model.** We consider $$x= \sum\_{i = 1}^{+\infty} h\_i^{1/2} z\_i u\_i,$$ where the variables \(z\_i \in \mathbb{R}\), \(i \geqslant 1\), are *independent* (and not merely un-correlated), and have zero mean and unit variance. The only parameter that we will need to characterize the operator \(T\) is the fourth-order moment, \(\mathbb{E} [ z\_i^2] = 3 + \kappa\), where \(\kappa\) is the [excess kurtosis](https://en.wikipedia.org/wiki/Kurtosis).

The Gaussian model [15] where all \(z\_i\) are Gaussians corresponds to \(\kappa=0\), while the Rademacher model where \(z\_i \in \{-1,1\}\) are [Rademacher](https://en.wikipedia.org/wiki/Rademacher_distribution) random variables corresponds to the smallest possible value \(\kappa=-2\). Other models could also be considered, particularly when using kernels [[25](https://openreview.net/pdf?id=bzaPGEllsjE)].

With this simplified model, as already shown in the 1980’s in the Gaussian context [[20](https://ieeexplore.ieee.org/abstract/document/236504), [15](https://ieeexplore.ieee.org/abstract/document/1164493)], in the basis of eigenvectors of \(H\), the operator \(T\) has a simple block-diagonal structure: the off-diagonal elements of \(\mathbb{E} \big[ \Theta\_k ] \) evolve independently (and converge to zero linearly), while the diagonal elements that are needed to compute the performance measure evolve according to a linear iteration with a matrix that is the sum of a diagonal and a rank-one matrix. More precisely, assuming without loss of generality that the operator \(H\) is diagonal equal to \({\rm Diag}(h)\), Eq. \((1)\) projected on diagonal elements becomes $${\rm diag}( \mathbb{E}[ \Theta\_k]) = ( I\, – \gamma V) {\rm diag}( \mathbb{E}[ \Theta\_k]) + \gamma^2 \sigma^2 h,$$ with $$V = {\rm Diag}\big( 2 h \, – \gamma(\kappa+2) h \circ h\big) – \gamma h \otimes h .$$

The performance \(a\_k = \mathbb{E} [ F(\eta\_k) ] – F(\eta\_\ast)\) is thus $$ a\_k = \frac{1}{2}\langle \delta \circ \delta, ( I \, – \gamma V )^k h \rangle + \gamma \sigma^2 \langle h, \big(I\, – ( I \, – \gamma V )^k \big) V^{-1} h \rangle. $$ In order to study convergence, we could look at an eigenvalue decomposition of the matrix \(V\) using “secular” equations [[21](https://epubs.siam.org/doi/pdf/10.1137/1015032)]. Instead, we will use the z-transform method, which avoids explicit computations of eigenvalues.

For simplicity, we consider here only the interpolation regime where \(\sigma^2=0\) (i.e., there is no noise on top of optimal predictions, see [[1](https://arxiv.org/pdf/2507.03404)] for all details). Then, the z-transform is equal to $$A(z) = \sum\_{k=0}^{+\infty} a\_k z^k = \frac{1}{2}\langle \delta \circ \delta, ( (1-z) I \, + z \gamma V )^{-1} h \rangle.$$ The matrix \((1-z) I \, + z \gamma V\) that needs to be inverted has a “diagonal + rank-one” structure, and can thus be inverted using the [matrix inversion lemma](https://en.wikipedia.org/wiki/Woodbury_matrix_identity), leading to an explicit rational function in \(z\), that can be directly analyzed around \(z = 1\) (see [[1](https://arxiv.org/pdf/2507.03404)]).

**Asymptotic equivalent.** Following [this post](https://francisbach.com/scaling-laws-of-optimization/) and the gradient descent analysis above, we consider \(h\_i = {L}/{i^\alpha}\) and \(\delta\_i = {\Delta}/{i^{\beta/2}}\). If we assume \(\kappa = -2\) (Rademacher case, which leads to the simplest formulas), the largest step-size is \(\gamma = 2/ {\rm tr}(H)\) (which can be significantly smaller than \(2 / \lambda\_\max(H)\) when \(\alpha\) is close to 1), and we get the asymptotic equivalent $$a\_k \sim \frac{L \Delta^2}{2 \alpha} \frac{1}{1-\gamma {\rm tr}(H)/2 } \frac{\Gamma(\omega)}{(2 \gamma L k)^{\omega}},$$ with \(\omega= (\beta-1)/\alpha + 1\), and under the condition \(\omega = (0, 2- 1/\alpha)\) (see [[1](https://arxiv.org/pdf/2507.03404)] for a result for all cases). Up to constants, this leads to the same equivalent as for gradient descent. See below for an illustration.

![](https://francisbach.com/wp-content/uploads/2025/07/video_lms_blog.gif)

Comparison of asymptotic equivalent and performance of SGD in the interpolation regime for several replications for \(\omega=1\).

We recover the results from [[22](https://openreview.net/pdf?id=bzaPGEllsjE)] with a simpler and more direct proof, thereby obtaining asymptotic results that complement the non-asymptotic results from [[23](https://proceedings.neurips.cc/paper_files/paper/2020/file/1b33d16fc562464579b7199ca3114982-Paper.pdf)]. This is also a subcase of results from [[24](https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf)] with a simpler proof. Various extensions are possible, e.g., using averaging of iterates, or computing estimates of the variance of the performance (and not only its expectation); see [[1](https://arxiv.org/pdf/2507.03404)] for details.

## Conclusion

In this blog post, I have shown how the z-transform can be used to derive asymptotic equivalents of sequences that naturally appear in the optimization of quadratic forms, with classical tools such as acceleration and stochastic approximation. In all cases, the z-transform method allows simple computations of asymptotic equivalents (e.g., scaling laws) through the expansion of rational functions.

Since quadratic functions are at the heart of optimization, many recent developments can be considered as candidates for the z-transform method, such as [Richardson extrapolation](https://francisbach.com/richardson-extrapolation/), [non-linear iterations for generic convex functions](https://francisbach.com/computer-aided-analyses/), time-varying coefficients beyond rational functions, mixing momentum techniques and SGD [[26](https://proceedings.mlr.press/v178/varre22a/varre22a.pdf), [26](https://arxiv.org/pdf/2505.16098)], primal-dual algorithms, [variance reduced methods](https://en.wikipedia.org/wiki/Stochastic_variance_reduction), or [sampling techniques](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm). Many situations where the z-transform can prove useful!

**Where does the “z” come from?** For those questioning the naming of the z-transform, the [wikipedia page](https://en.wikipedia.org/wiki/Z-transform) has some interesting historical notes. The idea of using generating functions to study sequences dates back at least to De Moivre around 1730, while its first use in discrete engineering systems was done by [Witold Hurewicz](https://fr.wikipedia.org/wiki/Witold_Hurewicz) in 1947 using the letter “z” (to make it different from the letter “s” used for the Laplace transform (its continuous-time counterpart). The name z-transform was later coined in 1952 by [John R. Ragazzini](https://en.wikipedia.org/wiki/John_R._Ragazzini) and [Lofti Zadeh](https://fr.wikipedia.org/wiki/Lotfi_Zadeh). As confirmed by a personal communication from Lofti Zadeh to [Murat Arcak](https://people.eecs.berkeley.edu/~arcak/), It has nothing to do with the first letter of Loftis Zadeh’s name (“I used the letter z not because of my name but because in the mathematical literature on difference equations, z was the symbol that was used”). However, I didn’t find any formal confirmation that [Zorro](http://Zorro) had nothing to do with it.

**Acknowledgements.** I thank Adrien Taylor, Ayoub Melliti, Nicolas Flammarion, Baptiste Goujaud, and Raphaël Berthier for insightful discussions related to this work.

![](https://francisbach.com/wp-content/uploads/2025/07/zorro.gif)

## References

[1] Francis Bach. [On the effectiveness of the z-transform method in quadratic optimization](https://arxiv.org/pdf/2507.03404). Technical report, arXiv:2507.03404, 2025.  
[2] Godfrey H. Hardy and John E. Littlewood. [Abel’s theorem and its converse](https://scholar.archive.org/work/6div4q4wvrfc5pheqboxzppbju/access/ia_file/crossref-pre-1923-scholarly-works/10.1112%252Fplms%252Fs2-10.1.116.zip/10.1112%252Fplms%252Fs2-18.1.205.pdf). *Proceedings of the London Mathematical Society*, 2(1):205–235, 1920.  
[3] Jacob Korevaar. *[Tauberian Theory: A Century of Developments](https://link.springer.com/book/10.1007/978-3-662-10225-1)*. Springer, 2004.  
[4] Philippe Flajolet and Andrew Odlyzko. [Singularity analysis of generating functions](https://algo.inria.fr/flajolet/Publications/FlOd90b.pdf). SIAM Journal on Discrete Mathematics, 3(2):216–240, 1990.  
[5] Edward A. Bender. [Asymptotic methods in enumeration](https://epubs.siam.org/doi/pdf/10.1137/1016082). *SIAM Review*, 16(4):485–515, 1974.  
[6] Eliahu Ibrahim Jury. *Theory and Application of the z-Transform Method*. Robert E. Krieger Publishing Company, 1964.  
[7] Herbert S. Wilf. *[Generatingfunctionology](https://www2.math.upenn.edu/~wilf/gfology2.pdf)*. CRC Press, 2005.  
[8] Raphaël Berthier, Francis Bach, and Pierre Gaillard. [Accelerated gossip in networks of given dimension using Jacobi polynomial iterations](https://arxiv.org/pdf/1805.08531). *SIAM Journal on Mathematics of Data Science*, 2(1):24–47, 2020.  
[9] Nicolas Flammarion and Francis Bach. [From averaging to acceleration, there is only a step-size](https://proceedings.mlr.press/v40/Flammarion15.pdf). *Conference on Learning Theory*, 2015.  
[10] David Vernon Widder. *Laplace Transform*. Princeton University Press, 1942.  
[11] Yurii Nesterov. [A method for solving a convex programming problem with rate of convergence \(O(1/k^2)\)](https://www.mathnet.ru/links/682668bd73f8d8866766f9356da2378e/dan46009.pdf). *Soviet Mathematics. Doklady*, 269(3):543–547, 1983.  
[12] Antonin Chambolle and Charles Dossal. [On the convergence of the iterates of the “fast iterative shrinkage/thresholding algorithm”](https://link.springer.com/content/pdf/10.1007/s10957-015-0746-4.pdf). *Journal of Optimization theory and Applications*, 166:968–982, 2015.  
[13] Jean-François Aujol, Charles Dossal, Hippolyte Labarrière, and Aude Rondepierre. [Strong convergence of FISTA iterates under Hölderian and quadratic growth conditions](https://arxiv.org/pdf/2407.17063). Technical Report 2407.17063, arXiv, 2024.  
[14] Neil Bershad. [Analysis of the normalized LMS algorithm with Gaussian inputs](https://ieeexplore.ieee.org/abstract/document/1164914). *IEEE Transactions on Acoustics, Speech, and Signal Processing*, 34(4):793–806, 1986.  
[15] Arie Feuer and Ehud Weinstein. [Convergence analysis of LMS filters with uncorrelated Gaussian data](https://ieeexplore.ieee.org/abstract/document/1164493). *IEEE Transactions on Acoustics, Speech, and Signal Processing*, 33(1):222–230, 2003.  
[16] Francis Bach and Eric Moulines. [Non-strongly-convex smooth stochastic approximation with convergence rate \(O(1/n)\)](https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf). *Advances in Neural Information Processing Systems*, 2013.  
[17] Alexandre Défossez and Francis Bach. [Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions](https://proceedings.mlr.press/v38/defossez15.pdf). *International Conference on Artificial Intelligence and Statistics*, 2015  
[18] Dirk T. M. Slock. [On the convergence behavior of the LMS and the normalized LMS algorithms](https://ieeexplore.ieee.org/document/236504). *IEEE Transactions on Signal Processing*, 41(9):2811–2825, 1993.  
[19] Frederik Kunstner, Francis Bach. [Scaling laws for gradient descent and sign descent for linear bigram models under Zipf’s law](https://arxiv.org/pdf/2505.19227). Technical report, arXiv:2505.19227, 2025.  
[20] Larry L. Horowitz and Kenneth D. Senne. [Performance advantage of complex LMS for controlling narrow-band adaptive arrays](https://ieeexplore.ieee.org/abstract/document/1085024). *IEEE Transactions on Circuits and Systems*, 28(6):562–576, 1981.  
[21] Gene H. Golub. [Some modified matrix eigenvalue problems](https://epubs.siam.org/doi/pdf/10.1137/1015032). *SIAM Review*, 15(2):318–334, 1973.  
[22] Maksim Velikanov, Denis Kuznedelev, and Dmitry Yarotsky. [A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta](https://openreview.net/pdf?id=bzaPGEllsjE). *International Conference on Learning Representations*, 2023.  
[23] Raphaël Berthier, Francis Bach, and Pierre Gaillard. [Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model](https://proceedings.neurips.cc/paper_files/paper/2020/file/1b33d16fc562464579b7199ca3114982-Paper.pdf). *Advances in Neural Information Processing Systems*, 2020b.  
[24] Elliot Paquette, Courtney Paquette, Lechao Xiao, and Jeffrey Pennington. [4+3 phases of compute-optimal neural scaling laws](https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf). *Advances in Neural Information Processing Systems*, 2024.  
[25] Aditya Varre and Nicolas Flammarion. [Accelerated SGD for non-strongly-convex least squares](https://proceedings.mlr.press/v178/varre22a/varre22a.pdf). In *Conference on Learning Theory*, 2022.  
[26] Damien Ferbach, Katie Everett, Gauthier Gidel, Elliot Paquette, and Courtney Paquette. [Dimension-adapted momentum outscales SGD](https://arxiv.org/pdf/2505.16098). Technical Report 2505.16098, arXiv, 2025.  
[27] Aymeric Dieuleveut, Nicolas Flammarion, Francis Bach. [Harder, better, faster, stronger convergence rates for least-squares regression](https://jmlr.org/papers/volume18/16-335/16-335.pdf). *Journal of Machine Learning Research*, 18(101):1-51, 2017.

By Francis Bach

[Read original post](https://francisbach.com/z-transform/)
