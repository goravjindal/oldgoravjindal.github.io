---
layout: post
category: cstheoryrss
title: "arXiv: Computational Complexity: Generalized and Unified Equivalences between Hardness and Pseudoentropy"
date: 2025-07-09T00:00:00
---

**Authors:** [Lunjia Hu](https://dblp.uni-trier.de/search?q=Lunjia+Hu), [Salil Vadhan](https://dblp.uni-trier.de/search?q=Salil+Vadhan)

Pseudoentropy characterizations provide a quantitatively precise
demonstration of the close relationship between computational hardness and
computational randomness. We prove a unified pseudoentropy characterization
that generalizes and strengthens previous results for both uniform and
non-uniform models of computation. Our characterization holds for a general
family of entropy notions that encompasses the common notions of Shannon
entropy and min entropy as special cases. Moreover, we show that the
characterizations for different entropy notions can be simultaneously achieved
by a single, universal function that simultaneously witnesses computational
hardness and computational randomness. A key technical insight of our work is
that the notion of weight-restricted calibration from the recent literature on
algorithm fairness, along with standard computational indistinguishability
(known as multiaccuracy in the fairness literature), suffices for proving
pseudoentropy characterizations for general entropy notions. This demonstrates
the power of weight-restricted calibration to enhance the classic
Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009)
and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to
achieve an exponential improvement in the complexity dependency on the alphabet
size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and
Vadhan (2024) based on the much stronger notion of multicalibration. We show
that the exponential dependency on the alphabet size is inevitable for
multicalibration as well as for the weaker notion of calibrated multiaccuracy.

[Read original post](http://arxiv.org/abs/2507.05972v1)
