<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Francis Bach: Revisiting scaling laws via the z-transform | Gorav  Jindal</title>
    <meta name="author" content="Gorav  Jindal">
    <meta name="description" content="Gorav Jindal's personal and academic webpage
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://goravjindal.github.io/_cstheoryrss/2025-07-18-francis-bach-revisiting-scaling-laws-via-the-z-transform.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Gorav </span>Jindal</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cstheoryrss/">CS Theory RSS</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Francis Bach: Revisiting scaling laws via the z-transform</h1>
    <p class="post-meta">July 18, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>In the last few years, we have seen a surge of empirical and theoretical works about “scaling laws”, whose goals are to characterize the performance of learning methods based on various problem parameters (e.g., number of observations and parameters, or amount of compute). From a theoretical point of view, this marks a renewed interest in asymptotic equivalents—something the machine learning community had mostly moved away from (and, let’s be honest, kind of looked down on) in favor of non-asymptotic bounds. The two types of bounds have their own pros and cons, and I value both of them, but on a personal note, after having spent close to twenty years in proving (sometimes tedious) non-asymptotic results, proving asymptotic results is particularly rewarding, especially when you experience that “aha!” moment where the empirical behavior aligns perfectly with the derived expression (see some examples below).</p>

<p>Scaling laws for quadratic optimization require aggregating the effects of all eigenvalues of the Hessian matrix and not considering only the worst one. This was done with Laplace’s method in an <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">earlier post</a> for gradient descent and partially for Nesterov acceleration. This blog post explores similar asymptotic scaling laws and will consider stochastic algorithms, but there is more to it.</p>

<p>I will use a classical tool from applied mathematics, electrical engineering, and computer science: the <em><a href="https://en.wikipedia.org/wiki/Z-transform" rel="external nofollow noopener" target="_blank">z-transform</a></em> (a.k.a. <a href="https://en.wikipedia.org/wiki/Generating_function" rel="external nofollow noopener" target="_blank">generating function</a>) of a discrete sequence. My goal in this blog post is to present the essentials of the method. For more details, see my recent preprint [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>], with lots of potential follow-ups. Note that the z-transform has already been used in the context of stochastic gradient descent for stability analysis [<a href="https://ieeexplore.ieee.org/abstract/document/1164493" rel="external nofollow noopener" target="_blank">15</a>, <a href="https://ieeexplore.ieee.org/abstract/document/236504" rel="external nofollow noopener" target="_blank">20</a>] and for deriving convergence rates with constant momentum [<a href="https://openreview.net/pdf?id=bzaPGEllsjE" rel="external nofollow noopener" target="_blank">22</a>].</p>

<h2 id="z-transform-method-and-final-value-theorem">z-transform method and final value theorem</h2>

<p><strong>From Abel’s theorem to the finite value theorem.</strong> Given a sequence ((b_k)_{k \geqslant 0}), its <a href="https://en.wikipedia.org/wiki/Z-transform" rel="external nofollow noopener" target="_blank">z-transform</a> is the function (B) defined for a complex argument (z \in \mathbb{C}), as (B(z) = \sum_{k=0}^\infty b_k z^k) (note the change of convention where (z^{-1}) is replaced by (z)). The behavior of the series defined by ((b_k)_{k \geqslant 0}) can be characterized by the behavior of (B(z)) for (z) tending to (1) while remaining in ([0,1)). This is the classical <a href="https://en.wikipedia.org/wiki/Abel%27s_theorem" rel="external nofollow noopener" target="_blank">Abel’s theorem</a>: when the series defined by ((b_k)_{k \geqslant 0}) is convergent, then \(\sum\_{k=0}^{+\infty} b\_k = \lim\_{z \to 1^-} B(z).\)</p>

<p>When applied to (b_k = a_k\, – a_{k-1}) for (k \geqslant 1), with (b_0 = a_0), then we have (\sum_{\ell=1}^k b_\ell = a_k), and (B(z) = (1\, – z) A(z)) where (A) is the z-transfom of ((a_k)_{k \geqslant 0}). We then obtain the <a href="https://en.wikipedia.org/wiki/Final_value_theorem" rel="external nofollow noopener" target="_blank">final value theorem</a>, which states \(\lim\_{z \to 1^-} \ ( 1 \, – z) A(z) = \lim\_{k \to +\infty} a\_k.\)</p>

<p><strong>From limits to asymptotic equivalents.</strong> In order to obtain equivalents of the sequence ((a_k)_{k \geqslant 0}) (and not simply its limit), we can use two additional properties:</p>

<ul>
  <li>the z-transform of (((k+1) a_{k+1})_{k \geqslant 0}) is (A’(z)), leading to \(\lim\_{z \to 1^-} \ ( 1 \, – z) A'(z)= \lim\_{k \to +\infty} k a\_k.\)</li>
  <li>To treat non-integer powers, we have the following identity, valid for any real (\nu &gt; 0), \(\lim\_{z \to 1^-} \ ( 1 \,- z)^{\nu} A(z) = \Gamma(\nu) \cdot \lim\_{k \to +\infty} k^{1-\nu} a\_k,\) where (\Gamma) is the <a href="https://en.wikipedia.org/wiki/Gamma_function" rel="external nofollow noopener" target="_blank">Gamma function</a>. A classical example is (a_k = \frac{1}{k!} \nu(\nu+1) \cdots (\nu+k-1) = \frac{ \Gamma(k+\nu)}{\Gamma(\nu)\Gamma(k+1)} \sim \frac{k^{\nu-1}}{\Gamma(\nu)}), for which (A(z) = (1-z)^{-\nu}).</li>
</ul>

<p>Combining the last two properties (with an extension to higher-order derivatives), when the two limits exist, we have: \(\lim\_{z \to 1^-} \ ( 1 \, – z)^{\nu} A^{(\mu)}(z) = \Gamma(\nu)\cdot \lim\_{k \to +\infty} k^{1+\mu-\nu} a\_k.\) Therefore, the z-transform method consists in deriving asymptotics of the z-transform or its derivatives around (z = 1^-). Before showing why this is a simplification, some words on sufficient conditions for the equivalence to hold, through a sequence of so-called <a href="https://en.wikipedia.org/wiki/Abelian_and_Tauberian_theorems" rel="external nofollow noopener" target="_blank">“Tauberian” theorems</a>.</p>

<p><strong>Tauberian theory.</strong> In order to show that limits and asymptotic equivalents exist, several options are available. When only considering real-valued arguments for the z-transforms, various sufficient conditions have been derived from the work of Hardy and Littlewood [<a href="https://scholar.archive.org/work/6div4q4wvrfc5pheqboxzppbju/access/ia_file/crossref-pre-1923-scholarly-works/10.1112%252Fplms%252Fs2-10.1.116.zip/10.1112%252Fplms%252Fs2-18.1.205.pdf" rel="external nofollow noopener" target="_blank">2</a>] (the book from Jacob Korevaar [3] is a great source of detailed results). In a nutshell, oscillating sequences such as (a_k = (-1)^k) are the ones where the limits are not equal (or may not exist), and a classical sufficient condition is that there exists a constant (c) such that for all (k \geqslant 1), (a_k \, – a_{k-1} \leqslant c / k^{2+\mu – \nu}) (in particular, with (c = 0), being decreasing is a sufficient condition). Alternative frameworks can also be considered using complex analysis, which only depends on property of the z-transform (A(z)) for complex (z) [<a href="https://algo.inria.fr/flajolet/Publications/FlOd90b.pdf" rel="external nofollow noopener" target="_blank">4</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/1016082" rel="external nofollow noopener" target="_blank">5</a>].</p>

<p><strong>Properties of the z-transform</strong>. What makes the z-transform so versatile is a suite of properties that link modifications of the sequence ((a_k)_{k \geqslant 0}) to its z-transform (A(z)) (see more details in [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>, 6]). The most important ones are:</p>

<ul>
  <li>
<strong>Shift:</strong> If (A(z)) is the z-transform of the sequence ((a_k)_{k \geqslant 0}), then (z A(z)) is the z-transform of the sequence ((0,a_0,a_1,\dots)), that is, shifted by 1. This implies that sequences that satisfy a <a href="https://en.wikipedia.org/wiki/Linear_recurrence_with_constant_coefficients" rel="external nofollow noopener" target="_blank">linear difference equation</a> lead to <em>rational</em> z-transforms.</li>
  <li>
<strong>Multiplication by a polynomial:</strong> If (A) is the z-transform of ((a_k)_{k \geqslant 0}), then (z \mapsto z A’(z)) is the (z)-transform of ((k a_k)_{k \geqslant 0}). Thus, when we have a difference equation with rational coefficients (in (k)), this leads to an ordinary differential equation for the z-transform.</li>
</ul>

<p><strong>Classical use of the z-transform method to derive asymptotic equivalents.</strong> The z-transform method is a classical tool, with already many <a href="https://en.wikipedia.org/wiki/Generating_function#Applications" rel="external nofollow noopener" target="_blank">applications</a>, e.g., in combinatorics, analysis of recurrence relations, queuing theory, signal processing, and control (see [6, <a href="https://www2.math.upenn.edu/~wilf/gfology2.pdf" rel="external nofollow noopener" target="_blank">7</a>]). In this blog post, I will focus on optimization of quadratic functions.</p>

<h2 id="gradient-descent-and-spectral-dimension">Gradient descent and spectral dimension</h2>

<p>Like in an <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">earlier post</a>, we consider an idealized quadratic optimization problem in infinite dimension, which can be rewritten as the minimization of \(F(\eta) = \frac{1}{2} \langle \eta-\eta\_\ast, H ( \eta-\eta\_\ast)\rangle,\) where (\eta_\ast) is a global optimum of (F) and (H) the positive semi-definite Hessian operator. Gradient descent leads to the recursion \(\eta\_k = \eta\_{k-1} \, – \gamma F'(\eta\_{k-1}) = \eta\_{k-1} \, – \gamma H ( \eta\_{k-1} \, – \eta\_\ast),\) where (\gamma &gt; 0) is a step-size which we choose so that the operator norm of (\gamma H) is less than one (e.g., (\gamma \leqslant 1/L), where (L) is the largest eigenvalue of the operator (H)). With (\theta_k = \eta_k \, – \eta_\ast), this leads to the following <em>linear</em> iteration \(\theta\_k = (I \, – \gamma H) \theta\_{k-1} = ( I \, – \gamma H)^k \delta,\) where (\delta = \theta_0 = \eta_0\, – \eta_\ast).</p>

<p>Thus, given a spectral decomposition (\gamma H = \sum_{i=1}^{+\infty} \lambda_i u_i \otimes u_i) with eigenvalues ((\lambda_i)_{i \geqslant 1}) in ([0,1]) and eigenvectors ((u_i)_{i \geqslant 1}), we get the following performance criterion \(a\_k = \frac{1}{2\gamma} \sum\_{i=1}^{+\infty} \lambda\_i (1 \, – \lambda\_i)^{2k} \langle u\_i,\delta \rangle^2 = \int\_{0}^1 (1 \, – \lambda)^{2k} d\sigma(\lambda),\) where \(d\sigma(\lambda) = \frac{1}{2\gamma} \sum\_{i=1}^{+\infty} \lambda\_i \langle u\_i,\delta \rangle^2 {\rm Dirac}(\lambda|\lambda\_i)\)<br>
is a weighted spectral measure, as defined in [<a href="https://arxiv.org/pdf/1805.08531" rel="external nofollow noopener" target="_blank">8</a>] in the gossip context (see this <a href="https://francisbach.com/jacobi-polynomials/" rel="external nofollow noopener" target="_blank">earlier post</a>), with a z-transform equal to \(A(z) = \sum\_{k=0}^{+\infty} z^k \int\_0^{1 } (1-\lambda)^{2k} d\sigma(\lambda)  
= \int\_0^{1 } \frac{1}{1 \, – z (1-\lambda)^{2}} d\sigma(\lambda),\) which is a rational function in (z) and (\lambda). It can thus be re-written as, using a partial function decomposition in the variable (\lambda), as \(A(z) = \frac{1}{2 \sqrt{z} } \int\_0^{1 } \frac{d\sigma(\lambda) }{ \lambda + z^{-1/2} \, – 1 } – \frac{1}{2\sqrt{z}} \int\_0^{1 } \frac{d\sigma(\lambda) }{ \lambda\, – z^{-1/2} – 1 }.\) We thus get \(A(z) = \frac{1}{2 \sqrt{z} } S( z^{-1/2}-1 ) – \frac{1}{2 \sqrt{z}}S ( – z^{-1/2}-1) ,\)<br>
where (\displaystyle S(u) = \int_0^1 ! \frac{d\sigma(\lambda)}{\lambda + u}) is the <a href="https://en.wikipedia.org/wiki/Stieltjes_transformation" rel="external nofollow noopener" target="_blank">Stieltjes transform</a> of the measure (\sigma) (see Chapter 8 of [10]), a classical tool in <a href="https://en.wikipedia.org/wiki/Random_matrix" rel="external nofollow noopener" target="_blank">random matrix theory</a>. We see above that what matters in the equivalent of (S(u)) around (u=0) (since the value at (-2) is bounded).</p>

<p>The <em>spectral dimension</em> defined by [<a href="https://arxiv.org/pdf/1805.08531" rel="external nofollow noopener" target="_blank">8</a>] corresponds to a measure whose behavior around zero has density (c \lambda^{\omega \, – 1}), or in almost equivalent terms so that (S(u) \sim c \Gamma(\omega) \Gamma(1-\omega) u^{\omega\, -1}) around (u=0), with a similar expansion for derivatives. This leads directly to a an explicit scaling law in ({1}/{k^\omega}) which exactly recovers an <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">earlier post</a> (which was using Laplace method) with a different proof.</p>

<p>The spectral dimension depends explicitly on problem parameters:</p>

<ul>
  <li>For least-squares regression, as described in an <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">earlier post</a> and in many papers, this corresponds to (\lambda_i \sim L / i^\alpha) (“capacity” conditions) and (\langle \delta, u_i \rangle \sim \Delta / i^{\beta/2}) (“source” conditions), leading to (\omega = \frac{\beta-1}{\alpha} + 1 ) assumed to be positive, and (c = \frac{L \Delta^2 }{2 \alpha (\gamma L)^{\frac{\beta-1}{\alpha}+1}}). The equivalent is then \(a\_ k \sim c \frac{\Gamma(\omega)}{(2k)^{\omega}},\) with nice <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">connections</a> with non-asymptotic bounds for the same problem.</li>
  <li>For gossip, as shown in [8], we get that (\omega) is the underlying dimension of the set on which local messages are sent.</li>
</ul>

<p><strong>Summary.</strong> Linear iterations lead to rational z-transforms along all eigenvectors, and then asymptotics of the <a href="https://en.wikipedia.org/wiki/Stieltjes_transformation" rel="external nofollow noopener" target="_blank">Stieltjes transform</a> (based on the spectral dimension) can be used after partial function decomposition. For gradient descent (a simple linear iteration with constant coefficients), this can be easily achieved in other ways. However, for time-dependent coefficients (Nesterov acceleration) and for stochastic iterations, it will provide a new direct way that avoids lengthy computations.</p>

<h2 id="nesterov-acceleration-vs-heavy-ball">Nesterov acceleration vs. heavy-ball</h2>

<p>A classical way to accelerate the convergence of the gradient iteration is to use an <a href="https://en.wikipedia.org/wiki/Gradient_descent#Fast_gradient_methods" rel="external nofollow noopener" target="_blank">extrapolation step</a> due to Nesterov [<a href="https://www.mathnet.ru/links/682668bd73f8d8866766f9356da2378e/dan46009.pdf" rel="external nofollow noopener" target="_blank">11</a>]. This can be formulated with two sequences (and a first-order recursion) as:<br>
\(\begin{array}{rcl} \eta\_{k+1} &amp; = &amp; \zeta\_{k} \, – \gamma F'(\zeta\_{k}) \\ \zeta\_{k+1} &amp; = &amp; \eta\_{k+1} + ( 1 \, – \tau\_{k+1}) ( \eta\_{k+1} \, – \eta\_k), \end{array}\) and with a single sequence (and a second-order recursion) as \(\eta\_{k+1} = \eta\_{k} + ( 1 \, – \tau\_{k}) ( \eta\_{k}\, – \eta\_{k-1})\, – \gamma F’ \big[ \eta\_{k} + ( 1 \,- \tau\_{k}) ( \eta\_{k} \, – \eta\_{k-1})\big].\)</p>

<p>In the original formulation with convex functions, (\tau_k) is chosen asymptotically proportional to (2\rho/(k+1)) with (\rho = 3/2). For quadratic functions, [<a href="https://proceedings.mlr.press/v40/Flammarion15.pdf" rel="external nofollow noopener" target="_blank">9</a>] argues that the choice (\rho = 1) is more natural as it leads to a simple rescaled reformulation, and [<a href="https://link.springer.com/content/pdf/10.1007/s10957-015-0746-4.pdf" rel="external nofollow noopener" target="_blank">12</a>, <a href="https://arxiv.org/pdf/2407.17063" rel="external nofollow noopener" target="_blank">13</a>] consider a general (\rho). We now show that the z-transform allows us to analyze all integers (\rho), with a conjecture that is empirically valid for all (\rho &gt;0).</p>

<p>With (\theta_k = \eta_k\, – \eta_\ast), we obtain the following recursion for quadratic functions \(\theta\_{k+1} = ( I- \gamma H) \bigg[ \theta\_{k} + \Big( 1\, – \frac{ 2\rho }{k+2\rho-1} \Big) ( \theta\_{k} \, – \theta\_{k-1}) \bigg].\) I show in [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>] how the performance measure leads to an explicit rational function in ((z,\lambda)) of order (2 \rho), obtained through differentiations of the z-transform (to take into account the rational depence of (\tau_k)). For (\rho=1), we can easily get by hand a partial function decomposition, while for larger integer (\rho), this can be readily done by symbolic computation tools such as Mathematica. Overall, the final equivalents for (a_k = \frac{1}{2} \langle \eta_k \, – \eta_\ast, H ( \eta_k \, – \eta_\ast) \rangle) are essentially (up to logarithmic terms) proportional to ({1}/{k^{\min{2\omega, \omega + \rho}}}). More precisely we get totally explicit equivalents:</p>

<ul>
  <li>(\displaystyle a_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot \frac{\Gamma(\rho-\omega) \Gamma(\omega)}{\Gamma(4\rho-1-2\omega)} \frac{\Gamma(2\rho-1/2-\omega)}{\Gamma(\rho+1/2-\omega)} \frac{2^{2\rho-1}}{4^\omega } \cdot \frac{1}{k^{ 2\omega}}) for (\omega \in (0,\rho)).</li>
  <li>(\displaystyle a_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot\frac{1}{2^{2\rho-1}} \cdot \frac{ \log k}{k^{2\rho}}) for (\omega = \rho).</li>
  <li>(\displaystyle a_k \sim c \frac{\Gamma(2 \rho)^2}{\Gamma(\rho)^2} \cdot \frac{1}{2^{2\rho-1}} \Gamma(\omega-\rho)\cdot \frac{1}{k^{\omega+\rho}}) for (\omega &gt; \rho).</li>
</ul>

<p>And amazingly, they do match in our experiments even beyond integer (\rho), as shown below.</p>

<p><img src="https://francisbach.com/wp-content/uploads/2025/07/video_nesterov_blog.gif" alt=""></p>

<p>Nesterov acceleration for (\omega = 1) and several (\rho)’s. Note the classical vanishing oscillations for small (\rho). Moreover, the best performance is achieved around (\rho \approx \omega).</p>

<p><strong>What about heavy-ball?</strong> The heavy-ball method is very similar to Nesterov acceleration, with the following iteration \(\eta\_{k+1} = \eta\_{k} \, – \gamma F'(\eta\_k) + (1-\tau\_{k+1}) ( \eta\_k \,- \eta\_{k-1}),\) leading to, with the same notations, to \(\theta\_{k+1} = \Big( I- \frac{k}{k+2\rho-1} \gamma H\Big) \theta\_{k} + \Big( 1 \, – \frac{ 2\rho }{k+2\rho-1} \Big) ( \theta\_{k} \, – \theta\_{k-1}).\) The associated z-transform is also the integral with respect to the spectral measure of a rational function, leading to an asymptotic equivalent around (z=1^-). Then, one can plot the actual performance and compare to the equivalent, as shown below.</p>

<p><img src="https://francisbach.com/wp-content/uploads/2025/07/heavyball_blog-1024x829.png" alt=""></p>

<p>Heavy-ball method with (\rho=1) for two values of the spectral dimension (\omega). Top: performance (a_k) vs. equivalent (\bar{a}_k), bottom: ratio.</p>

<p><em>What’s happening?</em> The two do not match for (\omega) large enough (right plot above). It turns out that oscillations do not vanish, and the sufficient Tauberian conditions are not satisfied. What can then be shown in terms of asymptotic equivalents remains open.</p>

<h2 id="stochastic-gradient-descent-aka-least-mean-squares-algorithm">Stochastic gradient descent (a.k.a. least-mean-squares algorithm)</h2>

<p>We now consider minimizing the expexted risk for a linear predictor with square loss: \(F(\eta) = \frac{1}{2}\mathbb{E} \big[ ( y \, – \langle x, \eta \rangle )^2 \big]\) (keeping in mind that we can use a feature vector). Single-pass <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="external nofollow noopener" target="_blank">stochastic gradient descent</a> (SGD) leads to the iteration \(\eta\_k = \eta\_{k-1}\, – \gamma ( \langle x\_k,\eta\_{k-1} \rangle \,- y\_k) x\_k,\) with ((x_k,y_k)) an independent sample from the distribution defining (F). We can write it using a minimizer (\eta_\ast) of (F) as \(\eta\_k -\eta\_\ast =( I \, – \gamma x\_k \otimes x\_k) (\eta\_{k-1} \, – \eta\_\ast) + \gamma \varepsilon\_k x\_k,\) where (\varepsilon_k = y_k \, – \langle x_k, \eta_\ast \rangle) is the optimal residual. We denote (\theta_k = \eta_k \, – \eta_\ast). In this context, SGD is referred to as the <a href="https://en.wikipedia.org/wiki/Least_mean_squares_filter" rel="external nofollow noopener" target="_blank">least-mean-squares</a> (LMS) algorithm [<a href="https://ieeexplore.ieee.org/abstract/document/1164914" rel="external nofollow noopener" target="_blank">14</a>, <a href="https://ieeexplore.ieee.org/abstract/document/1164493" rel="external nofollow noopener" target="_blank">15</a>, <a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf" rel="external nofollow noopener" target="_blank">16</a>, <a href="https://proceedings.mlr.press/v38/defossez15.pdf" rel="external nofollow noopener" target="_blank">17</a>].</p>

<p>The simplest but incomplete analysis uses that (\mathbb{E} [ \varepsilon_k x_k ] =0) (which is the optimality condition characterizing (\varepsilon_k)), to get: \(\mathbb{E}[\theta\_k] = ( I \, – \gamma H) \mathbb{E}[\theta\_{k-1}],\) leading to the classical linear iteration of gradient descent. However, it does not lead to any result on the performance measure (\frac{1}{2} \mathbb{E} [ \langle \theta_k, H \theta_k \rangle] = \mathbb{E} [ F(\eta_k)\, – F(\eta_\ast) ]).</p>

<table>
  <tbody>
    <tr>
      <td>Traditionally, there are two terms in the analysis: “bias” and “variance” (see a thorough discussion in [<a href="https://jmlr.org/papers/volume18/16-335/16-335.pdf" rel="external nofollow noopener" target="_blank">27</a>]). For simplicity, let’s assume that (\varepsilon_k) is independent of (x_k), with (\mathbb{E} [ \varepsilon_k]=0) and (\mathbb{E} [ \varepsilon_k^2]=\sigma^2) (not to be confused with the notation for the spectral measure defined above). We then consider the operator \(\Theta\_k = (\eta\_k \, – \eta\_\ast)\otimes (\eta\_k \,- \eta\_\ast) = \theta\_k \otimes \theta\_k,\) for which we have \(F(\eta\_k) \,- F(\eta\_\ast) = \frac{1}{2} \langle \Theta\_k,H \rangle = \frac{1}{2}{\rm tr}( \Theta\_k H),\) and $$ \mathbb{E} [ \Theta_k</td>
      <td>\Theta_{k-1}] = \mathbb{E} \big[ ( I \, – \gamma x_k \otimes x_k) \otimes ( I \, – \gamma x_k \otimes x_k) \big] \Theta_{k-1} + \gamma^2 \sigma^2 H,\(leading to:\) \mathbb{E} [ \Theta_k ] = \big( I\, – \gamma H \otimes I \, – \gamma I \otimes H\, + \gamma^2 \mathbb{E} [ x \otimes x \otimes x \otimes x] \big) \mathbb{E} [ \Theta_{k-1} ] \, + \gamma^2 \sigma^2 H.$$</td>
    </tr>
  </tbody>
</table>

<p>We consider the operator (defined on self-adjoint operators) \(T = H \otimes I \, + I \otimes H \, -\, \gamma \mathbb{E} [ x \otimes x \otimes x \otimes x],\) so that \(\tag{1} \mathbb{E} [ \Theta\_k ] = \big( I\, – \gamma T \big) \mathbb{E} [ \Theta\_{k-1} ] \, + \gamma^2 \sigma^2 H,\) leading to, after summation of the geometric series (see an <a href="https://francisbach.com/the-sum-of-a-geometric-series-is-all-you-need/" rel="external nofollow noopener" target="_blank">earlier post</a>): \(\mathbb{E} [ \Theta\_k ] = \big( I\, – \gamma T \big)^k \Theta\_{0} \, + \gamma \sigma^2 \big[ I- \big( I- \gamma T \big)^k \big] T^{-1} H .\)</p>

<p>A sufficient condition for convergence is that the operator (I\, – \gamma T) (as an operator on symmetric operators) has eigenvalues in ([-1,1]). In [<a href="https://proceedings.mlr.press/v38/defossez15.pdf" rel="external nofollow noopener" target="_blank">17</a>], we define and characterize the largest step-size with few assumptions. We consider instead specific models for asymptotic computations.</p>

<p><strong>Simplest model.</strong> In order to obtain simpler formulas, several models have been proposed that are compatible with the Hessian operator (H) having eigenvalues ((\lambda_i)_{i \geqslant 1}) and eigenvectors ((u_i)_{i \geqslant 1}). The simplest model [<a href="https://ieeexplore.ieee.org/document/236504" rel="external nofollow noopener" target="_blank">18</a>] is (x = r u_j) with (r \in \mathbb{R}) and (i \in \mathbb{N}^\ast) random and independent, with (\mathbb{E}[r^2] = \sum_{i \geqslant 1} h_i = {\rm tr}(H)), and (\mathbb{P}(j = i) = h_i / {\rm tr}(H)). What makes this model special is that, like in the deterministic case, there are no interactions between eigensubspaces. While this includes one-hot encoded discrete data as recently used in [<a href="https://arxiv.org/pdf/2505.19227" rel="external nofollow noopener" target="_blank">19</a>], this remains limited compared to the model below (which includes Gaussian data and is often used in random matrix theory).</p>

<p><strong>Simplified high-dimensional model.</strong> We consider \(x= \sum\_{i = 1}^{+\infty} h\_i^{1/2} z\_i u\_i,\) where the variables (z_i \in \mathbb{R}), (i \geqslant 1), are <em>independent</em> (and not merely un-correlated), and have zero mean and unit variance. The only parameter that we will need to characterize the operator (T) is the fourth-order moment, (\mathbb{E} [ z_i^2] = 3 + \kappa), where (\kappa) is the <a href="https://en.wikipedia.org/wiki/Kurtosis" rel="external nofollow noopener" target="_blank">excess kurtosis</a>.</p>

<p>The Gaussian model [15] where all (z_i) are Gaussians corresponds to (\kappa=0), while the Rademacher model where (z_i \in {-1,1}) are <a href="https://en.wikipedia.org/wiki/Rademacher_distribution" rel="external nofollow noopener" target="_blank">Rademacher</a> random variables corresponds to the smallest possible value (\kappa=-2). Other models could also be considered, particularly when using kernels [<a href="https://openreview.net/pdf?id=bzaPGEllsjE" rel="external nofollow noopener" target="_blank">25</a>].</p>

<p>With this simplified model, as already shown in the 1980’s in the Gaussian context [<a href="https://ieeexplore.ieee.org/abstract/document/236504" rel="external nofollow noopener" target="_blank">20</a>, <a href="https://ieeexplore.ieee.org/abstract/document/1164493" rel="external nofollow noopener" target="_blank">15</a>], in the basis of eigenvectors of (H), the operator (T) has a simple block-diagonal structure: the off-diagonal elements of (\mathbb{E} \big[ \Theta_k ] ) evolve independently (and converge to zero linearly), while the diagonal elements that are needed to compute the performance measure evolve according to a linear iteration with a matrix that is the sum of a diagonal and a rank-one matrix. More precisely, assuming without loss of generality that the operator (H) is diagonal equal to ({\rm Diag}(h)), Eq. ((1)) projected on diagonal elements becomes \({\rm diag}( \mathbb{E}[ \Theta\_k]) = ( I\, – \gamma V) {\rm diag}( \mathbb{E}[ \Theta\_k]) + \gamma^2 \sigma^2 h,\) with \(V = {\rm Diag}\big( 2 h \, – \gamma(\kappa+2) h \circ h\big) – \gamma h \otimes h .\)</p>

<p>The performance (a_k = \mathbb{E} [ F(\eta_k) ] – F(\eta_\ast)) is thus \(a\_k = \frac{1}{2}\langle \delta \circ \delta, ( I \, – \gamma V )^k h \rangle + \gamma \sigma^2 \langle h, \big(I\, – ( I \, – \gamma V )^k \big) V^{-1} h \rangle.\) In order to study convergence, we could look at an eigenvalue decomposition of the matrix (V) using “secular” equations [<a href="https://epubs.siam.org/doi/pdf/10.1137/1015032" rel="external nofollow noopener" target="_blank">21</a>]. Instead, we will use the z-transform method, which avoids explicit computations of eigenvalues.</p>

<p>For simplicity, we consider here only the interpolation regime where (\sigma^2=0) (i.e., there is no noise on top of optimal predictions, see [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>] for all details). Then, the z-transform is equal to \(A(z) = \sum\_{k=0}^{+\infty} a\_k z^k = \frac{1}{2}\langle \delta \circ \delta, ( (1-z) I \, + z \gamma V )^{-1} h \rangle.\) The matrix ((1-z) I \, + z \gamma V) that needs to be inverted has a “diagonal + rank-one” structure, and can thus be inverted using the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity" rel="external nofollow noopener" target="_blank">matrix inversion lemma</a>, leading to an explicit rational function in (z), that can be directly analyzed around (z = 1) (see [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>]).</p>

<p><strong>Asymptotic equivalent.</strong> Following <a href="https://francisbach.com/scaling-laws-of-optimization/" rel="external nofollow noopener" target="_blank">this post</a> and the gradient descent analysis above, we consider (h_i = {L}/{i^\alpha}) and (\delta_i = {\Delta}/{i^{\beta/2}}). If we assume (\kappa = -2) (Rademacher case, which leads to the simplest formulas), the largest step-size is (\gamma = 2/ {\rm tr}(H)) (which can be significantly smaller than (2 / \lambda_\max(H)) when (\alpha) is close to 1), and we get the asymptotic equivalent \(a\_k \sim \frac{L \Delta^2}{2 \alpha} \frac{1}{1-\gamma {\rm tr}(H)/2 } \frac{\Gamma(\omega)}{(2 \gamma L k)^{\omega}},\) with (\omega= (\beta-1)/\alpha + 1), and under the condition (\omega = (0, 2- 1/\alpha)) (see [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>] for a result for all cases). Up to constants, this leads to the same equivalent as for gradient descent. See below for an illustration.</p>

<p><img src="https://francisbach.com/wp-content/uploads/2025/07/video_lms_blog.gif" alt=""></p>

<p>Comparison of asymptotic equivalent and performance of SGD in the interpolation regime for several replications for (\omega=1).</p>

<p>We recover the results from [<a href="https://openreview.net/pdf?id=bzaPGEllsjE" rel="external nofollow noopener" target="_blank">22</a>] with a simpler and more direct proof, thereby obtaining asymptotic results that complement the non-asymptotic results from [<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1b33d16fc562464579b7199ca3114982-Paper.pdf" rel="external nofollow noopener" target="_blank">23</a>]. This is also a subcase of results from [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">24</a>] with a simpler proof. Various extensions are possible, e.g., using averaging of iterates, or computing estimates of the variance of the performance (and not only its expectation); see [<a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">1</a>] for details.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post, I have shown how the z-transform can be used to derive asymptotic equivalents of sequences that naturally appear in the optimization of quadratic forms, with classical tools such as acceleration and stochastic approximation. In all cases, the z-transform method allows simple computations of asymptotic equivalents (e.g., scaling laws) through the expansion of rational functions.</p>

<p>Since quadratic functions are at the heart of optimization, many recent developments can be considered as candidates for the z-transform method, such as <a href="https://francisbach.com/richardson-extrapolation/" rel="external nofollow noopener" target="_blank">Richardson extrapolation</a>, <a href="https://francisbach.com/computer-aided-analyses/" rel="external nofollow noopener" target="_blank">non-linear iterations for generic convex functions</a>, time-varying coefficients beyond rational functions, mixing momentum techniques and SGD [<a href="https://proceedings.mlr.press/v178/varre22a/varre22a.pdf" rel="external nofollow noopener" target="_blank">26</a>, <a href="https://arxiv.org/pdf/2505.16098" rel="external nofollow noopener" target="_blank">26</a>], primal-dual algorithms, <a href="https://en.wikipedia.org/wiki/Stochastic_variance_reduction" rel="external nofollow noopener" target="_blank">variance reduced methods</a>, or <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm" rel="external nofollow noopener" target="_blank">sampling techniques</a>. Many situations where the z-transform can prove useful!</p>

<p><strong>Where does the “z” come from?</strong> For those questioning the naming of the z-transform, the <a href="https://en.wikipedia.org/wiki/Z-transform" rel="external nofollow noopener" target="_blank">wikipedia page</a> has some interesting historical notes. The idea of using generating functions to study sequences dates back at least to De Moivre around 1730, while its first use in discrete engineering systems was done by <a href="https://fr.wikipedia.org/wiki/Witold_Hurewicz" rel="external nofollow noopener" target="_blank">Witold Hurewicz</a> in 1947 using the letter “z” (to make it different from the letter “s” used for the Laplace transform (its continuous-time counterpart). The name z-transform was later coined in 1952 by <a href="https://en.wikipedia.org/wiki/John_R._Ragazzini" rel="external nofollow noopener" target="_blank">John R. Ragazzini</a> and <a href="https://fr.wikipedia.org/wiki/Lotfi_Zadeh" rel="external nofollow noopener" target="_blank">Lofti Zadeh</a>. As confirmed by a personal communication from Lofti Zadeh to <a href="https://people.eecs.berkeley.edu/~arcak/" rel="external nofollow noopener" target="_blank">Murat Arcak</a>, It has nothing to do with the first letter of Loftis Zadeh’s name (“I used the letter z not because of my name but because in the mathematical literature on difference equations, z was the symbol that was used”). However, I didn’t find any formal confirmation that <a href="http://Zorro" rel="external nofollow noopener" target="_blank">Zorro</a> had nothing to do with it.</p>

<p><strong>Acknowledgements.</strong> I thank Adrien Taylor, Ayoub Melliti, Nicolas Flammarion, Baptiste Goujaud, and Raphaël Berthier for insightful discussions related to this work.</p>

<p><img src="https://francisbach.com/wp-content/uploads/2025/07/zorro.gif" alt=""></p>

<h2 id="references">References</h2>

<p>[1] Francis Bach. <a href="https://arxiv.org/pdf/2507.03404" rel="external nofollow noopener" target="_blank">On the effectiveness of the z-transform method in quadratic optimization</a>. Technical report, arXiv:2507.03404, 2025.<br>
[2] Godfrey H. Hardy and John E. Littlewood. <a href="https://scholar.archive.org/work/6div4q4wvrfc5pheqboxzppbju/access/ia_file/crossref-pre-1923-scholarly-works/10.1112%252Fplms%252Fs2-10.1.116.zip/10.1112%252Fplms%252Fs2-18.1.205.pdf" rel="external nofollow noopener" target="_blank">Abel’s theorem and its converse</a>. <em>Proceedings of the London Mathematical Society</em>, 2(1):205–235, 1920.<br>
[3] Jacob Korevaar. <em><a href="https://link.springer.com/book/10.1007/978-3-662-10225-1" rel="external nofollow noopener" target="_blank">Tauberian Theory: A Century of Developments</a></em>. Springer, 2004.<br>
[4] Philippe Flajolet and Andrew Odlyzko. <a href="https://algo.inria.fr/flajolet/Publications/FlOd90b.pdf" rel="external nofollow noopener" target="_blank">Singularity analysis of generating functions</a>. SIAM Journal on Discrete Mathematics, 3(2):216–240, 1990.<br>
[5] Edward A. Bender. <a href="https://epubs.siam.org/doi/pdf/10.1137/1016082" rel="external nofollow noopener" target="_blank">Asymptotic methods in enumeration</a>. <em>SIAM Review</em>, 16(4):485–515, 1974.<br>
[6] Eliahu Ibrahim Jury. <em>Theory and Application of the z-Transform Method</em>. Robert E. Krieger Publishing Company, 1964.<br>
[7] Herbert S. Wilf. <em><a href="https://www2.math.upenn.edu/~wilf/gfology2.pdf" rel="external nofollow noopener" target="_blank">Generatingfunctionology</a></em>. CRC Press, 2005.<br>
[8] Raphaël Berthier, Francis Bach, and Pierre Gaillard. <a href="https://arxiv.org/pdf/1805.08531" rel="external nofollow noopener" target="_blank">Accelerated gossip in networks of given dimension using Jacobi polynomial iterations</a>. <em>SIAM Journal on Mathematics of Data Science</em>, 2(1):24–47, 2020.<br>
[9] Nicolas Flammarion and Francis Bach. <a href="https://proceedings.mlr.press/v40/Flammarion15.pdf" rel="external nofollow noopener" target="_blank">From averaging to acceleration, there is only a step-size</a>. <em>Conference on Learning Theory</em>, 2015.<br>
[10] David Vernon Widder. <em>Laplace Transform</em>. Princeton University Press, 1942.<br>
[11] Yurii Nesterov. <a href="https://www.mathnet.ru/links/682668bd73f8d8866766f9356da2378e/dan46009.pdf" rel="external nofollow noopener" target="_blank">A method for solving a convex programming problem with rate of convergence (O(1/k^2))</a>. <em>Soviet Mathematics. Doklady</em>, 269(3):543–547, 1983.<br>
[12] Antonin Chambolle and Charles Dossal. <a href="https://link.springer.com/content/pdf/10.1007/s10957-015-0746-4.pdf" rel="external nofollow noopener" target="_blank">On the convergence of the iterates of the “fast iterative shrinkage/thresholding algorithm”</a>. <em>Journal of Optimization theory and Applications</em>, 166:968–982, 2015.<br>
[13] Jean-François Aujol, Charles Dossal, Hippolyte Labarrière, and Aude Rondepierre. <a href="https://arxiv.org/pdf/2407.17063" rel="external nofollow noopener" target="_blank">Strong convergence of FISTA iterates under Hölderian and quadratic growth conditions</a>. Technical Report 2407.17063, arXiv, 2024.<br>
[14] Neil Bershad. <a href="https://ieeexplore.ieee.org/abstract/document/1164914" rel="external nofollow noopener" target="_blank">Analysis of the normalized LMS algorithm with Gaussian inputs</a>. <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em>, 34(4):793–806, 1986.<br>
[15] Arie Feuer and Ehud Weinstein. <a href="https://ieeexplore.ieee.org/abstract/document/1164493" rel="external nofollow noopener" target="_blank">Convergence analysis of LMS filters with uncorrelated Gaussian data</a>. <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em>, 33(1):222–230, 2003.<br>
[16] Francis Bach and Eric Moulines. <a href="https://proceedings.neurips.cc/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf" rel="external nofollow noopener" target="_blank">Non-strongly-convex smooth stochastic approximation with convergence rate (O(1/n))</a>. <em>Advances in Neural Information Processing Systems</em>, 2013.<br>
[17] Alexandre Défossez and Francis Bach. <a href="https://proceedings.mlr.press/v38/defossez15.pdf" rel="external nofollow noopener" target="_blank">Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions</a>. <em>International Conference on Artificial Intelligence and Statistics</em>, 2015<br>
[18] Dirk T. M. Slock. <a href="https://ieeexplore.ieee.org/document/236504" rel="external nofollow noopener" target="_blank">On the convergence behavior of the LMS and the normalized LMS algorithms</a>. <em>IEEE Transactions on Signal Processing</em>, 41(9):2811–2825, 1993.<br>
[19] Frederik Kunstner, Francis Bach. <a href="https://arxiv.org/pdf/2505.19227" rel="external nofollow noopener" target="_blank">Scaling laws for gradient descent and sign descent for linear bigram models under Zipf’s law</a>. Technical report, arXiv:2505.19227, 2025.<br>
[20] Larry L. Horowitz and Kenneth D. Senne. <a href="https://ieeexplore.ieee.org/abstract/document/1085024" rel="external nofollow noopener" target="_blank">Performance advantage of complex LMS for controlling narrow-band adaptive arrays</a>. <em>IEEE Transactions on Circuits and Systems</em>, 28(6):562–576, 1981.<br>
[21] Gene H. Golub. <a href="https://epubs.siam.org/doi/pdf/10.1137/1015032" rel="external nofollow noopener" target="_blank">Some modified matrix eigenvalue problems</a>. <em>SIAM Review</em>, 15(2):318–334, 1973.<br>
[22] Maksim Velikanov, Denis Kuznedelev, and Dmitry Yarotsky. <a href="https://openreview.net/pdf?id=bzaPGEllsjE" rel="external nofollow noopener" target="_blank">A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta</a>. <em>International Conference on Learning Representations</em>, 2023.<br>
[23] Raphaël Berthier, Francis Bach, and Pierre Gaillard. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1b33d16fc562464579b7199ca3114982-Paper.pdf" rel="external nofollow noopener" target="_blank">Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model</a>. <em>Advances in Neural Information Processing Systems</em>, 2020b.<br>
[24] Elliot Paquette, Courtney Paquette, Lechao Xiao, and Jeffrey Pennington. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/1dccfc3ee01871d05e33457c61037d59-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">4+3 phases of compute-optimal neural scaling laws</a>. <em>Advances in Neural Information Processing Systems</em>, 2024.<br>
[25] Aditya Varre and Nicolas Flammarion. <a href="https://proceedings.mlr.press/v178/varre22a/varre22a.pdf" rel="external nofollow noopener" target="_blank">Accelerated SGD for non-strongly-convex least squares</a>. In <em>Conference on Learning Theory</em>, 2022.<br>
[26] Damien Ferbach, Katie Everett, Gauthier Gidel, Elliot Paquette, and Courtney Paquette. <a href="https://arxiv.org/pdf/2505.16098" rel="external nofollow noopener" target="_blank">Dimension-adapted momentum outscales SGD</a>. Technical Report 2505.16098, arXiv, 2025.<br>
[27] Aymeric Dieuleveut, Nicolas Flammarion, Francis Bach. <a href="https://jmlr.org/papers/volume18/16-335/16-335.pdf" rel="external nofollow noopener" target="_blank">Harder, better, faster, stronger convergence rates for least-squares regression</a>. <em>Journal of Machine Learning Research</em>, 18(101):1-51, 2017.</p>

<p>By Francis Bach</p>

<p><a href="https://francisbach.com/z-transform/" rel="external nofollow noopener" target="_blank">Read original post</a></p>

    </div>
  </article>


</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Gorav  Jindal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
