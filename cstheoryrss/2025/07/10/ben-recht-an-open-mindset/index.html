<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ben Recht: An open mindset | Gorav  Jindal</title>
    <meta name="author" content="Gorav  Jindal">
    <meta name="description" content="Gorav Jindal's personal and academic webpage
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://goravjindal.github.io/cstheoryrss/2025/07/10/ben-recht-an-open-mindset/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Gorav </span>Jindal</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cstheoryrss/">CS Theory RSS</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Ben Recht: An open mindset</h1>
    <p class="post-meta">July 10, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/category/cstheoryrss">
          <i class="fas fa-tag fa-sm"></i> cstheoryrss</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p><a href="https://substackcdn.com/image/fetch/$s_!cNNz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26acc5c2-4af2-4f99-9d35-e7fa57ff6dd2_1100x219.jpeg" rel="external nofollow noopener" target="_blank"><img src="https://substackcdn.com/image/fetch/$s_!cNNz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26acc5c2-4af2-4f99-9d35-e7fa57ff6dd2_1100x219.jpeg" alt=""></a></p>

<p>Over the 4th of July weekend, Nathan Lambert wrote a thoughtful post on “<a href="https://www.interconnects.ai/p/the-american-deepseek-project" rel="external nofollow noopener" target="_blank">an American Deepseek Project</a>,” posing the challenge of building a fully open, fully performant “frontier model” in the next two years. Deepseek, in case you’ve already forgotten, is a Chinese company that released a highly performing, open-source large language model chatbot in January that could be trained from scratch for under <a href="https://imgflip.com/memegenerator/16799330/one-million-dollars" rel="external nofollow noopener" target="_blank">10 million dollars</a>. Nathan challenges the US research community to produce a similar open artifact.</p>

<p>Nathan’s post reminded me of what happened the last time we were told that you needed to be a hyperscaling tech company to do machine learning. In 2014, a year after paying <a href="https://www.wired.com/story/secret-auction-race-ai-supremacy-google-microsoft-baidu/" rel="external nofollow noopener" target="_blank">44 million dollars</a> for Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton, Google won the ILSVRC competition with their “GoogLeNet” model, achieving 93% top-5 accuracy.<a href="https://theory.report/atom.xml#footnote-1" rel="external nofollow noopener" target="_blank">1</a> While the authors didn’t release the full details of how to train this model, they asserted they used Google’s internal DistBelief system, a distributed computing framework for wrangling thousands of Google CPUs. However, based on a back of the envelope calculation, <a href="https://arxiv.org/pdf/1409.4842v1" rel="external nofollow noopener" target="_blank">they claimed</a> “ the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage.”</p>

<p>Shortly thereafter, a team at Berkeley sort of reproduced this result, showing that half a day with 128 GPUs could get to <a href="https://arxiv.org/abs/1511.00175" rel="external nofollow noopener" target="_blank">88% top-5 accuracy with the GoogLeNet architecture</a>. I imagine they could have reached 93% had they run for longer. A supercomputer with 128 GPUs was still a serious investment for any academic team, and openly trained ImageNet models still seemed to require infrastructural investments outside the reach of most labs.</p>

<p>In 2015, a team of researchers at Microsoft Research Asia in Beijing <a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">won the ILSVRC competition with their architecture called the ResNet</a>. ResNets were interesting because the principles of architecture were easier to understand. The repeated layers in ResNets made it easy to explore and reimplement the architecture.</p>

<p>Soon thereafter, the Dawn project at Stanford opened a competition, called <a href="https://dawn.cs.stanford.edu/dawnbench" rel="external nofollow noopener" target="_blank">DawnBench</a>, to see who could train ResNets the fastest. In November 2017, their baseline entry used 8 GPUs for 10.5 days to train a ResNet ImageNet classifier with 93% top-5 accuracy. This cost $1100 using Amazon cloud services. Already, this was getting somewhere, but I don’t think the Dawn team anticipated how quickly competition progress would come. By April 2018, that is, in less than six months, <a href="https://dawnd9.sites.stanford.edu/news/dawnbench-v1-deep-learning-benchmark-results" rel="external nofollow noopener" target="_blank">competitors had the training time down to 30 minutes on a single TPU core on Google Cloud</a>. That’s nearly a 500x acceleration in 6 months, and a dramatic cost reduction.</p>

<p>I tell this story because I’m optimistic that Nathan’s challenge is feasible. For two decades, Google and its peers have argued that you need scale to do anything. These claims are marketing. They have been proven wrong several times,<a href="https://theory.report/atom.xml#footnote-2" rel="external nofollow noopener" target="_blank">2</a> but it required a significant community commitment to do it. What are those commitments for Nathan’s Deepseek Project?</p>

<p>First, it’s elevating old school computational evaluations. During the initial deep learning boom, the machine learning research community decided that worthy <a href="https://hdsr.mitpress.mit.edu/pub/g9mau4m0/release/2" rel="external nofollow noopener" target="_blank">common tasks</a> were training time and model accessibility. These evaluations have been missing from the latest “frontier models” arms race. It’s also understandable that the computing for “frontier models” is much larger than it was for ImageNet. The hyperscalers’ only advantage is to find a starting point that’s inaccessible to a researcher without an infinite cloud computing budget. The 6 million dollars needed to train DeepSeek is out of reach for almost any machine learning researcher. This means that the initial work on any fully open LLM will require large-scale collaboration or perhaps an industrial benefactor. On the other hand, a 500x improvement in training time gets these models down to costing under twenty thousand dollars, which is the sort of thing you can add as a line item in a grant proposal.</p>

<p>More importantly, I want to emphasize that Nathan’s challenge is for something greater than just GPU golf. When he writes “open model,” he really means <em>open</em>. The Deepseek model is “open weights,” which means you can run and tweak it, but you can’t build it from scratch. This is similar to the openness of Meta’s Llama models. Nathan’s vision calls for open data, training code, logs, and decision making too.</p>

<p>I will never get over how the entire machine learning community is writing papers about systems where they don’t know what the training data is. Yes, the artifacts are interesting, but no, we’re not going to learn anything by fine-tuning RL models on top and running hackneyed 1960s psychology experiments on computers. Certainly, open weight models are preferable to closed weight models. But we shouldn’t have to settle.</p>

<p>Open data poses its own challenges to our community. Though we generally concede that open data has been the primary means of driving machine learning research, parts of the research community are openly hostile to open data. And it’s not exactly who you’d think it would be. Some of the biggest “success stories” of the responsible AI/fairness community are based on finding “dangerous” content in open data sets, resulting in these data sets being removed from the public domain. The Netflix Prize, Tiny Images Dataset, and yes, even ImageNet, have all been either entirely removed, taken down for extended periods, or neutered in some capacity because of highly questionable arguments about “responsibility.”</p>

<p>This push towards prioritizing academic notions of “privacy,” “safety,” and “fairness” over all else has mostly resulted in <em>academic</em> censorship. And yet, while the hyperscalers love to pay lip service to responsible behavior, they don’t stop using this data! Recent lawsuits have revealed that <a href="https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/" rel="external nofollow noopener" target="_blank">Anthropic scans millions of books</a> and <a href="https://futurism.com/the-byte/facebook-trained-ai-pirated-books" rel="external nofollow noopener" target="_blank">Facebook trains on LibGen</a>. But academic researchers aren’t allowed to use the Tiny Images Dataset. The hyperscalers do whatever they want, in private, locking out competition and strengthening their oligopoly. They know they will find a favorable ruling in <a href="https://www.theverge.com/2017/10/19/16503076/oracle-vs-google-judge-william-alsup-interview-waymo-uber" rel="external nofollow noopener" target="_blank">Bill Alsup’s</a> court because they have infinite money for their legal teams.</p>

<p>So while it’s undeniably essential to raise issues about the societal impacts of machine learning technology, we have to be careful not to do this in a way that solidifies corporate power. If we want fully open and accessible machine learning models, we need to figure out how to build them while accepting that the internet on which they are trained is littered with <a href="https://arstechnica.com/tech-policy/2024/08/nonprofit-scrubs-illegal-content-from-controversial-ai-training-dataset/" rel="external nofollow noopener" target="_blank">endlessly horrific and dangerous content</a>. Nathan’s challenge is thus asking the open research community to engage with uncomfortable trade-offs about what open <em>means.</em> This question is as vital as shaking the can to raise money for NVIDIA chips.</p>

<p><a href="https://www.argmin.net/subscribe" rel="external nofollow noopener" target="_blank">Subscribe now</a></p>

<p><a href="https://theory.report/atom.xml#footnote-anchor-1" rel="external nofollow noopener" target="_blank">1</a></p>

<p>For the purposes of this blog post “top-5 accuracy” is just a number between 0 and 100. Higher is better. Don’t sweat the details of what this means or actually evaluates.</p>

<p><a href="https://theory.report/atom.xml#footnote-anchor-2" rel="external nofollow noopener" target="_blank">2</a></p>

<p>Naomi Saphra points me to another <a href="https://arxiv.org/abs/2311.05020" rel="external nofollow noopener" target="_blank">example in machine translation</a>.</p>

<p>By Ben Recht</p>

<p><a href="https://www.argmin.net/p/an-open-mindset" rel="external nofollow noopener" target="_blank">Read original post</a></p>

    </div>
  </article>


</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Gorav  Jindal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
