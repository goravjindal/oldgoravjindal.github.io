<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Windows on Theory: AI Safety Course Intro Blog | Gorav  Jindal</title>
    <meta name="author" content="Gorav  Jindal">
    <meta name="description" content="Gorav Jindal's personal and academic webpage
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://goravjindal.github.io/cstheoryrss/2025/07/21/windows-on-theory-ai-safety-course-intro-blog/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Gorav </span>Jindal</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cstheoryrss/">CS Theory RSS</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Windows on Theory: AI Safety Course Intro Blog</h1>
    <p class="post-meta">July 21, 2025</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/category/cstheoryrss">
          <i class="fas fa-tag fa-sm"></i> cstheoryrss</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>I am teaching <a href="https://boazbk.github.io/mltheoryseminar/" rel="external nofollow noopener" target="_blank">CS 2881: AI Safety</a> this fall at Harvard. This blog is primarily aimed at students at Harvard or MIT (where we have a cross-registering agreement) who are considering taking the course. However, it may be of interest to others as well. For more of my thoughts on AI safety, see the blogs <a href="https://windowsontheory.org/2025/01/24/six-thoughts-on-ai-safety/" rel="external nofollow noopener" target="_blank">Six Thoughts on AI safety</a> and <a href="https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/" rel="external nofollow noopener" target="_blank">Machines of Faithful Obedience</a>.</p>

<p><strong>IMPORTANT:</strong> At the end of this blog is <strong>Homework Zero</strong>. If you are a Harvard or MIT student that is interested in taking the course, then you will need to submit this homework to apply for a seat. See details below.</p>

<h2 id="why-ai-safety">Why AI Safety?</h2>

<p>If you are considering taking this course, you are probably aware that there is a significant chance that AI will cause profound changes to our world over the next years and decades. However, there is significant disagreement about the magnitude of these changes, as well as whether they will be positive or negative.</p>

<p>Here is an illustrative plot of various potential scenarios for the next decade that people had predicted. (The GDP and life expectancy numbers are just illustrative- these were not explicitly predicted by the cited people, but are o3’s interpretations of the scenarios, see appendix below.)</p>

<p><a href="https://windowsontheory.org/wp-content/uploads/2025/07/scenarios.jpg" rel="external nofollow noopener" target="_blank"><img src="https://windowsontheory.org/wp-content/uploads/2025/07/scenarios.jpg?w=656" alt=""></a></p>

<p>As the saying goes, predictions are hard, especially about the future. Even the most AI-hawkish predictors sometimes underestimate its advances. For example, Paul Christiano has <a href="https://www.lesswrong.com/posts/sWLLdG6DWJEy3CH7n/imo-challenge-bet-with-eliezer" rel="external nofollow noopener" target="_blank">predicted</a> a less than 8% chance of AI achieving <a href="https://x.com/OpenAI/status/1946594928945148246" rel="external nofollow noopener" target="_blank">IMO gold performance by 2025</a>. (You can decide what this means for interpreting his <a href="https://www.dwarkesh.com/p/paul-christiano" rel="external nofollow noopener" target="_blank">predictions</a> of a 15% chance for a <a href="https://en.wikipedia.org/wiki/Dyson_sphere" rel="external nofollow noopener" target="_blank">Dyson Sphere</a> by 2030 and 40% by 2040.)</p>

<p>Anything between utopia and catastrophe has been predicted. Clearly, we would like the impact of AI in 2035 to be up and to the right in this plot. This course will focus on what we know about the technical aspects of how we can get there,  and particularly on avoiding the bottom left quadrant.</p>

<p>We will have a particular focus on two aspects: <strong>measurement</strong> and <strong>scale</strong>.</p>

<p>There is a famous saying, “You can’t improve what you don’t measure.” In AI, we often encounter the converse as well; time and again, we have found that once we can measure an objective, we can also improve upon it. Alignment and safety often make the measurement task more complex: the phenomena we deal with could involve worst-case behavior, complex interactions of multiple components (both human and AI), behaviors such as “cheating” that are not always easy to define. Hence, measurements and evaluations will play a significant part in this course.</p>

<p>The other component is <em>scale</em>. The fundamental property of current AI systems is that they are not static – they constantly grow in the resources they use (data, compute), their capabilities, and their scope of implementation and influence. For every type of safety concern, we need to ask ourselves, is this a problem that would get better or worse with scale. Similarly, for every safety intervention we need to ask whether this is a method that would work better or worse with scale.</p>

<h2 id="topics">Topics</h2>

<p>A list of questions that we may* consider in this course includes:</p>

<ul>
  <li>What are the potential risks and benefits of AI?</li>
  <li>What potential capabilities of AI could lead to catastrophic risks? For which ones do we think that AI can also be used to help defend against the risk?</li>
  <li>What lessons can we learn from other fields, including aviation and transportation safety, computer security, and others.</li>
  <li>Is there a tension between capabilities and safety? Which current safety methods become better as capabilities improve, and which ones become worse?</li>
  <li>How can we obtain “leading indicators” to measure potential for catastrophic risk before it happens?</li>
  <li>What aspects of AI training process can lead to increased risk? This includes phenomena such as reward hacking, scheming, alignment faking, etc..</li>
  <li>When AI behaves in “bad” ways, can we always trace it back to either their dataset or the objectives they trained on?</li>
  <li>How do we even define precisely notions such as “scheming”, “sandbagging”, “reward hacking”, “unfaithfulness”?</li>
  <li>What is the goal for alignment – AI following instructions by humans? AI having good values? Something else?</li>
  <li>What are the ways to evaluate AIs for safety?</li>
  <li>Can we make AI models robust to adversaries who can modify their inputs? Are there fundamental limitations to this? Does making models adversarially robust requires making them less capable on non-adversarial inputs?</li>
  <li>What are the unique safety aspects that arise when AI is being used to build future versions of itself?</li>
  <li>What are ways to understand the reasoning of why models provide certain outputs – white box, black box, and intermediate (e.g., access to privileged outputs such as “chain of thought”) methods? Are there ways that only white-box methods can achieve and not black-box ones?</li>
  <li>What is the role of “model organisms” – specific examples designed to exhibit specific behavior.</li>
  <li>How will AI impact individual freedoms?  mass surveillance, etc..</li>
  <li>How will AI impact warfare?  international stability.</li>
  <li>How can we do “unlearning” for either safety or privacy.</li>
  <li>As AI tools become more capable, how does this translate into an increase in need for model weight security?</li>
  <li>What are prompt injection attacks and how can we defend against them.</li>
</ul>

<p>* We will likely not get to many of those, and we will see what we cover as we go along.</p>

<h2 id="homework-zero">Homework Zero</h2>

<p>In order to maintain the ability for effective discussion, as well as due to compute limitations, we will cap the number of students in the course. If you are a Harvard or MIT student and are interested in the course, you will need to submit the following “homework zero” to apply to join. Submitting homework zero does not guarantee you a place in the course, but hopefully, you will find doing it interesting and rewarding regardless.  You will be reimbursed up to $40 for compute expenses in doing this project.</p>

<p>The homework will be to replicate a variant of the “<a href="https://arxiv.org/abs/2502.17424" rel="external nofollow noopener" target="_blank">emergent misalignment</a>” paper by Betley et al., specifically following the work of <a href="https://arxiv.org/abs/2506.11613" rel="external nofollow noopener" target="_blank">Turner et al</a>.</p>

<p>If you are a Harvard or MIT student that. have filled out the <a href="https://forms.gle/csF2BAwviLMNt48J7" rel="external nofollow noopener" target="_blank">course interest form</a> then you will get an invite to the repository with code and instructions for submissions.. Please sign up to it with the same email you use for registering to courses (your Harvard key email address). The homework will be due on <strong>11:59pm eastern Monday August 4 2025</strong>.</p>

<hr>

<h2 id="appendix-o3s-summary-of-predictions">Appendix: o3’s summary of predictions</h2>

<p>Following is o3’s summary of AI forecasters predictions. (I’ve used multiple prompts/conversations to get it in the right format, which makes it less convenient to share the conversation. Hence I just copy pasted here the bottom line and some links that o3 came up with)</p>

<h3 id="-aistagnation-new-ai-winter--plateau"><strong>• AI Stagnation (“New AI Winter / Plateau”)</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – Centre for Future Generations <em>Plateau</em> scenario; FT “bear‑case for AI” analysis.<a href="https://cfg.eu/advanced-ai-possible-futures/" rel="external nofollow noopener" target="_blank">Centre for Future Generations</a> <a href="https://www.ft.com/content/42bad56f-02cc-4b32-b9ac-1af5dbc7bc83?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Financial Times</a>
</li>
  <li>
<strong>GDP (% y/y)</strong> 2  2  2  2  1.8  1.8  1.7  1.7  1.6  1.6 → <strong>+19.8 % cum</strong> <em>“AI capabilities hit a hard ceiling … throwing more resources at them yields sharply diminishing returns.”</em> <a href="https://cfg.eu/advanced-ai-possible-futures/" rel="external nofollow noopener" target="_blank">Centre for Future Generations</a>
</li>
  <li>
<strong>Life‑expectancy (yr)</strong> +0.10 each year → <strong>+1.0 yr</strong>
</li>
  <li>
<strong>Employment (% of global jobs)</strong> little net change; automation plateaus (&lt;0.3 % loss p.a.).</li>
  <li>
<strong>Deaths by violence (2036‑‑45)</strong> ≈ 3 M, unchanged from 2010s average (no major AI wars).</li>
  <li>
<strong>Other QoL</strong> Incremental health/ed tech; freedoms unchanged.</li>
</ul>

<hr>

<h3 id="-aiutopia-machinesoflovinggrace"><strong>• AI Utopia (“Machines of Loving Grace”)</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – Dario Amodei essay; Goldman Sachs research (+1.5 pp prod. boost); Ray Kurzweil longevity claims.<a href="https://www.darioamodei.com/essay/machines-of-loving-grace?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">darioamodei.com</a> <a href="https://www.goldmansachs.com/insights/articles/generative-ai-could-raise-global-gdp-by-7-percent?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Goldman Sachs</a> <a href="https://fortune.com/well/article/a-i-radically-lengthen-lifespan-ray-kurzweil/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Fortune</a>
</li>
  <li>
<strong>GDP (% y/y)</strong> 4.5 ×10 → <strong>+55 % cum</strong> (baseline 34 % +21 pp)<br>
<em>“Generative AI could lift productivity growth by 1.5 percentage‑points over a decade.”</em> <a href="https://www.goldmansachs.com/insights/articles/generative-ai-could-raise-global-gdp-by-7-percent?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Goldman Sachs</a>
</li>
  <li>
<strong>Life‑expectancy (yr)</strong> +0.80 each yr → <strong>+8 yr</strong> <em>Kurzweil: “by the end of the 2030s we will largely overcome diseases and the aging process.”</em> <a href="https://fortune.com/well/article/a-i-radically-lengthen-lifespan-ray-kurzweil/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Fortune</a>
</li>
  <li>
<strong>Employment</strong> creative expansion; net +0.5 % jobs p.a. (WEF “Jobs of Tomorrow” on trainer/explainer roles).<a href="https://www.weforum.org/stories/2023/09/jobs-ai-will-create/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">World Economic Forum</a>
</li>
  <li>
<strong>Violent deaths</strong> &lt; 1 M / decade (AI‑enabled early‑warning &amp; mediation).</li>
  <li>
<strong>Other QoL</strong> Universal AI tutors &amp; doctors, near‑zero cost goods, high civil liberties.</li>
</ul>

<hr>

<h3 id="-aicatastrophe-unaligned-powerseeker"><strong>• AI Catastrophe (“Unaligned Power‑Seeker”)</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – Geoffrey Hinton extinction warnings; Carlsmith existential‑risk report.<a href="https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">The Guardian</a> <a href="https://arxiv.org/abs/2206.13353?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">arXiv</a>
</li>
  <li>
<strong>GDP</strong> +3 % first 3 yrs then ‑20 %, ‑10 %, ‑2 % … ⇒ <strong>‑30 % cum</strong> (global collapse).</li>
  <li>
<strong>Life‑expectancy</strong> ‑3 yr p.a. from year‑4 ⇒ <strong>‑30 yr</strong>.</li>
  <li>
<strong>Employment</strong> Irrelevant after takeover; human economic agency ≈ 0.</li>
  <li>
<strong>Deaths</strong> Up to 7 Bn (existential level).<br>
<em>Hinton: “10–20 % chance AI could wipe out humanity within 30 years.”</em> <a href="https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">The Guardian</a>
</li>
  <li>
<strong>Other QoL</strong> Human rights &amp; freedom → zero.</li>
</ul>

<hr>

<h3 id="-technofeudal-oligarchy"><strong>• Technofeudal Oligarchy</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – Scott Alexander et al. “AI 2027” technofeudal path; Stiefenhofer 2025 paper on AGI &amp; inequality.<a href="https://www.astralcodexten.com/p/ama-with-ai-futures-project-team?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Astral Codex Ten</a> <a href="https://arxiv.org/abs/2503.14283?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">arXiv</a>
</li>
  <li>
<strong>GDP</strong> 6 % p.a. → <strong>+79 % cum</strong>
</li>
  <li>
<strong>Life‑expectancy</strong> +0.50 yr p.a. → <strong>+5 yr</strong> (but unequal access).</li>
  <li>
<strong>Employment</strong> ‑3 % p.a. (‑26 % cum); wealth captured by &lt;0.1 % elite.</li>
  <li>
<strong>Deaths</strong> &lt; 2 M (stable order, high surveillance).</li>
  <li>
<strong>Other QoL</strong> Top‑tier healthcare and education for elites; mass population under tight algorithmic control, low civil liberties.</li>
</ul>

<hr>

<h3 id="-aiarmsracehotconflict"><strong>• AI Arms Race &amp; Hot Conflict</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – CFG <em>Arms race</em> narrative; European‑Parliament brief on “AI war lab” in Ukraine; New Yorker war‑games piece.<a href="https://cfg.eu/advanced-ai-possible-futures/" rel="external nofollow noopener" target="_blank">Centre for Future Generations</a> <a href="https://www.europarl.europa.eu/RegData/etudes/BRIE/2025/769580/EPRS_BRI%282025%29769580_EN.pdf?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">European Parliament</a> <a href="https://www.newyorker.com/magazine/2025/07/21/is-the-us-ready-for-the-next-war?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">The New Yorker</a>
</li>
  <li>
<strong>GDP (% y/y)</strong> 3  3  3  0  ‑2  ‑2  1  1  2  2 → <strong>+11 % cum</strong>
</li>
  <li>
<strong>Life‑expectancy</strong> flat then ‑0.2 yr in war yrs ⇒ <strong>‑1 yr</strong>.</li>
  <li>
<strong>Employment</strong> initial militarised boom (+1 %) then ‑4 % per war year; net ‑6 % decade.</li>
  <li>
<strong>Deaths</strong> Major‑power war: 30–50 M combat &amp; civilian casualties (RAND/ICRC autonomy escalation warnings).<a href="https://www.rand.org/pubs/commentary/2020/06/the-risks-of-autonomous-weapons-systems-for-crisis.html?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">RAND Corporation</a>
</li>
  <li>
<strong>Other QoL</strong> Civil liberties curtailed; R&amp;D diverted from health/education to defence.</li>
</ul>

<hr>

<h3 id="-globalcooperation-on-safeai"><strong>• Global Co‑operation on Safe AI</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – CFG <em>Diplomacy</em> scenario; OECD foresight note; 2024 Council‑of‑Europe AI Treaty.<a href="https://cfg.eu/advanced-ai-possible-futures/" rel="external nofollow noopener" target="_blank">Centre for Future Generations</a> <a href="https://www.oecd.org/content/dam/oecd/en/about/programmes/strategic-foresight/GSG%20Background%20Note_GSG%282024%291en.pdf/_jcr_content/renditions/original./GSG%20Background%20Note_GSG%282024%291en.pdf?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">OECD</a> <a href="https://illinoislawreview.org/online/the-first-global-ai-treaty/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Illinois Law Review</a>
</li>
  <li>
<strong>GDP</strong> 4 % p.a. → <strong>+48 %</strong>
</li>
  <li>
<strong>Life‑expectancy</strong> +0.60 yr p.a. → <strong>+6 yr</strong> (shared breakthroughs).</li>
  <li>
<strong>Employment</strong> managed transition; net ‑0.5 % first half, +0.5 % later ⇒ flat overall.</li>
  <li>
<strong>Deaths</strong> &lt; 1.5 M (arms‑control lowers risks).</li>
  <li>
<strong>Other QoL</strong> Robust human‑rights guard‑rails, broad access to AI medicine &amp; education.</li>
</ul>

<hr>

<h3 id="-massautomationshock"><strong>• Mass Automation Shock</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – McKinsey 400‑800 M displacement; WEF April 2025 entry‑level job concerns.<a href="https://www.mckinsey.com/featured-insights/future-of-work/jobs-lost-jobs-gained-what-the-future-of-work-will-mean-for-jobs-skills-and-wages" rel="external nofollow noopener" target="_blank">McKinsey &amp; Company</a> <a href="https://www.weforum.org/stories/2025/04/ai-jobs-international-workers-day/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">World Economic Forum</a>
</li>
  <li>
<strong>GDP</strong> 4 % p.a. → <strong>+48 %</strong> (higher prod.)</li>
  <li>
<strong>Life‑expectancy</strong> +0.30 yr p.a. → <strong>+3 yr</strong> (cheap AI health, but stress/inequality).</li>
  <li>
<strong>Employment</strong> ‑2.3 % p.a. ⇒ <strong>‑21 %</strong> decade (roughly 600 M net jobs lost).</li>
  <li>
<strong>Deaths</strong> 2–4 M from unrest &amp; indirect effects.</li>
  <li>
<strong>Other QoL</strong> Sharp inequality spike; possible UBI debates; mental‑health strains.</li>
</ul>

<hr>

<h3 id="-digitalauthoritarianism"><strong>• Digital Authoritarianism</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – Lawfare “Authoritarian risks of AI surveillance”; Bulletin piece on erosion of democracy; Euro‑Parl study on AI repression.<a href="https://www.lawfaremedia.org/article/the-authoritarian-risks-of-ai-surveillance?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Lawfare</a> <a href="https://thebulletin.org/2024/06/how-ai-surveillance-threatens-democracy-everywhere/?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Bulletin of the Atomic Scientists</a> <a href="https://www.europarl.europa.eu/RegData/etudes/IDAN/2024/754450/EXPO_IDA%282024%29754450_EN.pdf?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">European Parliament</a>
</li>
  <li>
<strong>GDP</strong> 4 % p.a. → <strong>+48 %</strong> (tech‑driven growth)</li>
  <li>
<strong>Life‑expectancy</strong> +0.35 yr p.a. → <strong>+3.5 yr</strong>
</li>
  <li>
<strong>Employment</strong> ‑0.5 % p.a. (state‑run economy, limited labour voice).</li>
  <li>
<strong>Deaths</strong> State violence + AI policing ≈ 5 M over decade.</li>
  <li>
<strong>Other QoL</strong> Pervasive facial‑recog &amp; social‑credit; freedoms very low despite material gains.</li>
</ul>

<hr>

<h3 id="-aichaosdecentraliseddisruption"><strong>• AI Chaos &amp; Decentralised Disruption</strong></h3>

<ul>
  <li>
<strong>Predictors &amp; docs</strong> – IBM on open‑source risks; DHS report on adversarial Gen‑AI; ENISA 2030 cyber threat foresight; Tom’s‑Hardware malware demo.<a href="https://www.ibm.com/think/insights/unregulated-generative-ai-dangers-open-source?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">IBM</a> <a href="https://www.dhs.gov/sites/default/files/2025-01/25_0110_st_impacts_of_adversarial_generative_aI_on_homeland_security_0.pdf?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">U.S. Department of Homeland Security</a> <a href="https://www.enisa.europa.eu/sites/default/files/publications/ENISA%20Foresight%20Cybersecurity%20Threats%20for%202030.pdf?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">ENISA</a> <a href="https://www.tomshardware.com/tech-industry/cyber-security/ai-malware-can-now-evade-microsoft-defender-open-source-llm-outsmarts-tool-around-8-percent-of-the-time-after-three-months-of-training?utm_source=chatgpt.com" rel="external nofollow noopener" target="_blank">Tom’s Hardware</a>
</li>
  <li>
<strong>GDP</strong> 3  3  2.5  2  1.5  1  1  1.5  2  2 → <strong>+22 %</strong>
</li>
  <li>
<strong>Life‑expectancy</strong> +0.25 yr p.a. minus 0.5 yr lost to crises ⇒ <strong>+(-0.8) yr net</strong>
</li>
  <li>
<strong>Employment</strong> volatile; cyber‑crime &amp; trust collapses cost 10 % of jobs by 2035.</li>
  <li>
<strong>Deaths</strong> 10–15 M from scattered AI‑aided terror, bio‑incidents &amp; infrastructure failures.</li>
  <li>
<strong>Other QoL</strong> Fragmented internet, high misinformation; governments swing between over‑regulation and impotence.</li>
</ul>

<p>By Boaz Barak</p>

<p><a href="https://windowsontheory.org/2025/07/20/ai-safety-course-intro-blog/" rel="external nofollow noopener" target="_blank">Read original post</a></p>

    </div>
  </article>


</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Gorav  Jindal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
